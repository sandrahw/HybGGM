SBATCH job
Started 05/11/2024 23:17:50
Working directory /eejit/home/hausw001/HybGGM/hybGGM_test
UNet6 with 0.4 0.3 10 0.0001 64 large start at Tue Nov  5 23:17:52 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 10 learning_rate: 0.0001 batch_size: 64 model_type: UNet6
Epoch 1/10
Epoch [1/10], Training Loss: 0.0545, Validation Loss: 0.0456
Best model saved!
Epoch 2/10
Epoch [2/10], Training Loss: 0.0513, Validation Loss: 0.0455
Best model saved!
Epoch 3/10
Epoch [3/10], Training Loss: 0.0495, Validation Loss: 0.0455
Best model saved!
Epoch 4/10
Epoch [4/10], Training Loss: 0.0479, Validation Loss: 0.0455
Best model saved!
Epoch 5/10
Epoch [5/10], Training Loss: 0.0462, Validation Loss: 0.0454
Best model saved!
Epoch 6/10
Epoch [6/10], Training Loss: 0.0448, Validation Loss: 0.0454
Best model saved!
Epoch 7/10
Epoch [7/10], Training Loss: 0.0435, Validation Loss: 0.0454
Best model saved!
Epoch 8/10
Epoch [8/10], Training Loss: 0.0424, Validation Loss: 0.0453
Best model saved!
Epoch 9/10
Epoch [9/10], Training Loss: 0.0412, Validation Loss: 0.0452
Best model saved!
Epoch 10/10
Epoch [10/10], Training Loss: 0.0412, Validation Loss: 0.0451
Best model saved!
Test Loss: 0.0339
done with hyperparameter tuning training
Model type: UNet6, Epochs: 10, Learning rate: 0.0001, Batch size: 64
Hyperparameter tuning prediction finished
UNet6 with 0.4 0.3 10 0.0001 64 large done at Wed Nov  6 00:36:03 CET 2024
UNet6 with 0.4 0.3 50 0.0001 64 large start at Wed Nov  6 00:36:03 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 50 learning_rate: 0.0001 batch_size: 64 model_type: UNet6
Epoch 1/50
Epoch [1/50], Training Loss: 0.0545, Validation Loss: 0.0456
Best model saved!
Epoch 2/50
Epoch [2/50], Training Loss: 0.0513, Validation Loss: 0.0455
Best model saved!
Epoch 3/50
Epoch [3/50], Training Loss: 0.0495, Validation Loss: 0.0455
Best model saved!
Epoch 4/50
Epoch [4/50], Training Loss: 0.0479, Validation Loss: 0.0455
Best model saved!
Epoch 5/50
Epoch [5/50], Training Loss: 0.0462, Validation Loss: 0.0454
Best model saved!
Epoch 6/50
Epoch [6/50], Training Loss: 0.0448, Validation Loss: 0.0454
Best model saved!
Epoch 7/50
Epoch [7/50], Training Loss: 0.0435, Validation Loss: 0.0454
Best model saved!
Epoch 8/50
Epoch [8/50], Training Loss: 0.0424, Validation Loss: 0.0453
Best model saved!
Epoch 9/50
Epoch [9/50], Training Loss: 0.0412, Validation Loss: 0.0452
Best model saved!
Epoch 10/50
Epoch [10/50], Training Loss: 0.0412, Validation Loss: 0.0451
Best model saved!
Epoch 11/50
Epoch [11/50], Training Loss: 0.0399, Validation Loss: 0.0450
Best model saved!
Epoch 12/50
Epoch [12/50], Training Loss: 0.0402, Validation Loss: 0.0450
Best model saved!
Epoch 13/50
Epoch [13/50], Training Loss: 0.0389, Validation Loss: 0.0448
Best model saved!
Epoch 14/50
Epoch [14/50], Training Loss: 0.0387, Validation Loss: 0.0447
Best model saved!
Epoch 15/50
Epoch [15/50], Training Loss: 0.0380, Validation Loss: 0.0446
Best model saved!
Epoch 16/50
Epoch [16/50], Training Loss: 0.0372, Validation Loss: 0.0444
Best model saved!
Epoch 17/50
Epoch [17/50], Training Loss: 0.0367, Validation Loss: 0.0442
Best model saved!
Epoch 18/50
Epoch [18/50], Training Loss: 0.0361, Validation Loss: 0.0439
Best model saved!
Epoch 19/50
Epoch [19/50], Training Loss: 0.0356, Validation Loss: 0.0437
Best model saved!
Epoch 20/50
Epoch [20/50], Training Loss: 0.0349, Validation Loss: 0.0435
Best model saved!
Epoch 21/50
Epoch [21/50], Training Loss: 0.0345, Validation Loss: 0.0431
Best model saved!
Epoch 22/50
Epoch [22/50], Training Loss: 0.0340, Validation Loss: 0.0427
Best model saved!
Epoch 23/50
Epoch [23/50], Training Loss: 0.0335, Validation Loss: 0.0422
Best model saved!
Epoch 24/50
Epoch [24/50], Training Loss: 0.0336, Validation Loss: 0.0418
Best model saved!
Epoch 25/50
Epoch [25/50], Training Loss: 0.0336, Validation Loss: 0.0411
Best model saved!
Epoch 26/50
Epoch [26/50], Training Loss: 0.0344, Validation Loss: 0.0408
Best model saved!
Epoch 27/50
Epoch [27/50], Training Loss: 0.0332, Validation Loss: 0.0405
Best model saved!
Epoch 28/50
Epoch [28/50], Training Loss: 0.0329, Validation Loss: 0.0398
Best model saved!
Epoch 29/50
Epoch [29/50], Training Loss: 0.0326, Validation Loss: 0.0390
Best model saved!
Epoch 30/50
Epoch [30/50], Training Loss: 0.0322, Validation Loss: 0.0385
Best model saved!
Epoch 31/50
Epoch [31/50], Training Loss: 0.0321, Validation Loss: 0.0383
Best model saved!
Epoch 32/50
Epoch [32/50], Training Loss: 0.0317, Validation Loss: 0.0382
Best model saved!
Epoch 33/50
Epoch [33/50], Training Loss: 0.0314, Validation Loss: 0.0378
Best model saved!
Epoch 34/50
Epoch [34/50], Training Loss: 0.0310, Validation Loss: 0.0372
Best model saved!
Epoch 35/50
Epoch [35/50], Training Loss: 0.0308, Validation Loss: 0.0368
Best model saved!
Epoch 36/50
Epoch [36/50], Training Loss: 0.0305, Validation Loss: 0.0364
Best model saved!
Epoch 37/50
Epoch [37/50], Training Loss: 0.0301, Validation Loss: 0.0361
Best model saved!
Epoch 38/50
Epoch [38/50], Training Loss: 0.0299, Validation Loss: 0.0356
Best model saved!
Epoch 39/50
Epoch [39/50], Training Loss: 0.0296, Validation Loss: 0.0351
Best model saved!
Epoch 40/50
Epoch [40/50], Training Loss: 0.0293, Validation Loss: 0.0350
Best model saved!
Epoch 41/50
Epoch [41/50], Training Loss: 0.0290, Validation Loss: 0.0348
Best model saved!
Epoch 42/50
Epoch [42/50], Training Loss: 0.0287, Validation Loss: 0.0346
Best model saved!
Epoch 43/50
Epoch [43/50], Training Loss: 0.0284, Validation Loss: 0.0341
Best model saved!
Epoch 44/50
Epoch [44/50], Training Loss: 0.0282, Validation Loss: 0.0342
Epoch 45/50
Epoch [45/50], Training Loss: 0.0280, Validation Loss: 0.0333
Best model saved!
Epoch 46/50
Epoch [46/50], Training Loss: 0.0284, Validation Loss: 0.0330
Best model saved!
Epoch 47/50
Epoch [47/50], Training Loss: 0.0278, Validation Loss: 0.0333
Epoch 48/50
Epoch [48/50], Training Loss: 0.0279, Validation Loss: 0.0321
Best model saved!
Epoch 49/50
Epoch [49/50], Training Loss: 0.0274, Validation Loss: 0.0321
Epoch 50/50
Epoch [50/50], Training Loss: 0.0271, Validation Loss: 0.0316
Best model saved!
Test Loss: 0.0221
done with hyperparameter tuning training
Model type: UNet6, Epochs: 50, Learning rate: 0.0001, Batch size: 64
Hyperparameter tuning prediction finished
UNet6 with 0.4 0.3 50 0.0001 64 large done at Wed Nov  6 06:41:02 CET 2024
UNet6 with 0.4 0.3 100 0.0001 64 large start at Wed Nov  6 06:41:02 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 100 learning_rate: 0.0001 batch_size: 64 model_type: UNet6
Epoch 1/100
Epoch [1/100], Training Loss: 0.0545, Validation Loss: 0.0456
Best model saved!
Epoch 2/100
Epoch [2/100], Training Loss: 0.0513, Validation Loss: 0.0455
Best model saved!
Epoch 3/100
Epoch [3/100], Training Loss: 0.0495, Validation Loss: 0.0455
Best model saved!
Epoch 4/100
Epoch [4/100], Training Loss: 0.0479, Validation Loss: 0.0455
Best model saved!
Epoch 5/100
Epoch [5/100], Training Loss: 0.0462, Validation Loss: 0.0454
Best model saved!
Epoch 6/100
Epoch [6/100], Training Loss: 0.0448, Validation Loss: 0.0454
Best model saved!
Epoch 7/100
Epoch [7/100], Training Loss: 0.0435, Validation Loss: 0.0454
Best model saved!
Epoch 8/100
Epoch [8/100], Training Loss: 0.0424, Validation Loss: 0.0453
Best model saved!
Epoch 9/100
Epoch [9/100], Training Loss: 0.0412, Validation Loss: 0.0452
Best model saved!
Epoch 10/100
Epoch [10/100], Training Loss: 0.0412, Validation Loss: 0.0451
Best model saved!
Epoch 11/100
Epoch [11/100], Training Loss: 0.0399, Validation Loss: 0.0450
Best model saved!
Epoch 12/100
Epoch [12/100], Training Loss: 0.0402, Validation Loss: 0.0450
Best model saved!
Epoch 13/100
Epoch [13/100], Training Loss: 0.0389, Validation Loss: 0.0448
Best model saved!
Epoch 14/100
Epoch [14/100], Training Loss: 0.0387, Validation Loss: 0.0447
Best model saved!
Epoch 15/100
Epoch [15/100], Training Loss: 0.0380, Validation Loss: 0.0446
Best model saved!
Epoch 16/100
Epoch [16/100], Training Loss: 0.0372, Validation Loss: 0.0444
Best model saved!
Epoch 17/100
Epoch [17/100], Training Loss: 0.0367, Validation Loss: 0.0442
Best model saved!
Epoch 18/100
Epoch [18/100], Training Loss: 0.0361, Validation Loss: 0.0439
Best model saved!
Epoch 19/100
Epoch [19/100], Training Loss: 0.0356, Validation Loss: 0.0437
Best model saved!
Epoch 20/100
Epoch [20/100], Training Loss: 0.0349, Validation Loss: 0.0435
Best model saved!
Epoch 21/100
Epoch [21/100], Training Loss: 0.0345, Validation Loss: 0.0431
Best model saved!
Epoch 22/100
Epoch [22/100], Training Loss: 0.0340, Validation Loss: 0.0427
Best model saved!
Epoch 23/100
Epoch [23/100], Training Loss: 0.0335, Validation Loss: 0.0422
Best model saved!
Epoch 24/100
Epoch [24/100], Training Loss: 0.0336, Validation Loss: 0.0418
Best model saved!
Epoch 25/100
Epoch [25/100], Training Loss: 0.0336, Validation Loss: 0.0411
Best model saved!
Epoch 26/100
Epoch [26/100], Training Loss: 0.0344, Validation Loss: 0.0408
Best model saved!
Epoch 27/100
Epoch [27/100], Training Loss: 0.0332, Validation Loss: 0.0405
Best model saved!
Epoch 28/100
Epoch [28/100], Training Loss: 0.0329, Validation Loss: 0.0398
Best model saved!
Epoch 29/100
Epoch [29/100], Training Loss: 0.0326, Validation Loss: 0.0390
Best model saved!
Epoch 30/100
Epoch [30/100], Training Loss: 0.0322, Validation Loss: 0.0385
Best model saved!
Epoch 31/100
Epoch [31/100], Training Loss: 0.0321, Validation Loss: 0.0383
Best model saved!
Epoch 32/100
Epoch [32/100], Training Loss: 0.0317, Validation Loss: 0.0382
Best model saved!
Epoch 33/100
Epoch [33/100], Training Loss: 0.0314, Validation Loss: 0.0378
Best model saved!
Epoch 34/100
Epoch [34/100], Training Loss: 0.0310, Validation Loss: 0.0372
Best model saved!
Epoch 35/100
Epoch [35/100], Training Loss: 0.0308, Validation Loss: 0.0368
Best model saved!
Epoch 36/100
Epoch [36/100], Training Loss: 0.0305, Validation Loss: 0.0364
Best model saved!
Epoch 37/100
Epoch [37/100], Training Loss: 0.0301, Validation Loss: 0.0361
Best model saved!
Epoch 38/100
Epoch [38/100], Training Loss: 0.0299, Validation Loss: 0.0356
Best model saved!
Epoch 39/100
Epoch [39/100], Training Loss: 0.0296, Validation Loss: 0.0351
Best model saved!
Epoch 40/100
Epoch [40/100], Training Loss: 0.0293, Validation Loss: 0.0350
Best model saved!
Epoch 41/100
Epoch [41/100], Training Loss: 0.0290, Validation Loss: 0.0348
Best model saved!
Epoch 42/100
Epoch [42/100], Training Loss: 0.0287, Validation Loss: 0.0346
Best model saved!
Epoch 43/100
Epoch [43/100], Training Loss: 0.0284, Validation Loss: 0.0341
Best model saved!
Epoch 44/100
Epoch [44/100], Training Loss: 0.0282, Validation Loss: 0.0342
Epoch 45/100
Epoch [45/100], Training Loss: 0.0280, Validation Loss: 0.0333
Best model saved!
Epoch 46/100
Epoch [46/100], Training Loss: 0.0284, Validation Loss: 0.0330
Best model saved!
Epoch 47/100
Epoch [47/100], Training Loss: 0.0278, Validation Loss: 0.0333
Epoch 48/100
Epoch [48/100], Training Loss: 0.0279, Validation Loss: 0.0321
Best model saved!
Epoch 49/100
Epoch [49/100], Training Loss: 0.0274, Validation Loss: 0.0321
Epoch 50/100
Epoch [50/100], Training Loss: 0.0271, Validation Loss: 0.0316
Best model saved!
Epoch 51/100
Epoch [51/100], Training Loss: 0.0269, Validation Loss: 0.0312
Best model saved!
Epoch 52/100
Epoch [52/100], Training Loss: 0.0265, Validation Loss: 0.0310
Best model saved!
Epoch 53/100
Epoch [53/100], Training Loss: 0.0263, Validation Loss: 0.0309
Best model saved!
Epoch 54/100
Epoch [54/100], Training Loss: 0.0258, Validation Loss: 0.0308
Best model saved!
Epoch 55/100
Epoch [55/100], Training Loss: 0.0258, Validation Loss: 0.0302
Best model saved!
Epoch 56/100
Epoch [56/100], Training Loss: 0.0253, Validation Loss: 0.0302
Epoch 57/100
Epoch [57/100], Training Loss: 0.0252, Validation Loss: 0.0300
Best model saved!
Epoch 58/100
Epoch [58/100], Training Loss: 0.0248, Validation Loss: 0.0297
Best model saved!
Epoch 59/100
Epoch [59/100], Training Loss: 0.0246, Validation Loss: 0.0298
Epoch 60/100
Epoch [60/100], Training Loss: 0.0243, Validation Loss: 0.0297
Best model saved!
Epoch 61/100
Epoch [61/100], Training Loss: 0.0240, Validation Loss: 0.0296
Best model saved!
Epoch 62/100
Epoch [62/100], Training Loss: 0.0238, Validation Loss: 0.0297
Epoch 63/100
Epoch [63/100], Training Loss: 0.0237, Validation Loss: 0.0309
Epoch 64/100
Epoch [64/100], Training Loss: 0.0240, Validation Loss: 0.0306
Epoch 65/100
Epoch [65/100], Training Loss: 0.0239, Validation Loss: 0.0289
Best model saved!
Epoch 66/100
Epoch [66/100], Training Loss: 0.0232, Validation Loss: 0.0291
Epoch 67/100
Epoch [67/100], Training Loss: 0.0232, Validation Loss: 0.0285
Best model saved!
Epoch 68/100
Epoch [68/100], Training Loss: 0.0227, Validation Loss: 0.0285
Best model saved!
Epoch 69/100
Epoch [69/100], Training Loss: 0.0227, Validation Loss: 0.0276
Best model saved!
Epoch 70/100
Epoch [70/100], Training Loss: 0.0223, Validation Loss: 0.0282
Epoch 71/100
Epoch [71/100], Training Loss: 0.0222, Validation Loss: 0.0279
Epoch 72/100
Epoch [72/100], Training Loss: 0.0221, Validation Loss: 0.0283
Epoch 73/100
Epoch [73/100], Training Loss: 0.0221, Validation Loss: 0.0277
Epoch 74/100
Epoch [74/100], Training Loss: 0.0216, Validation Loss: 0.0278
Epoch 75/100
Epoch [75/100], Training Loss: 0.0215, Validation Loss: 0.0276
Epoch 76/100
Epoch [76/100], Training Loss: 0.0212, Validation Loss: 0.0277
Epoch 77/100
Epoch [77/100], Training Loss: 0.0212, Validation Loss: 0.0272
Best model saved!
Epoch 78/100
Epoch [78/100], Training Loss: 0.0209, Validation Loss: 0.0277
Epoch 79/100
Epoch [79/100], Training Loss: 0.0207, Validation Loss: 0.0272
Best model saved!
Epoch 80/100
Epoch [80/100], Training Loss: 0.0206, Validation Loss: 0.0269
Best model saved!
Epoch 81/100
Epoch [81/100], Training Loss: 0.0203, Validation Loss: 0.0266
Best model saved!
Epoch 82/100
Epoch [82/100], Training Loss: 0.0200, Validation Loss: 0.0268
Epoch 83/100
Epoch [83/100], Training Loss: 0.0199, Validation Loss: 0.0267
Epoch 84/100
Epoch [84/100], Training Loss: 0.0198, Validation Loss: 0.0266
Best model saved!
Epoch 85/100
Epoch [85/100], Training Loss: 0.0196, Validation Loss: 0.0265
Best model saved!
Epoch 86/100
Epoch [86/100], Training Loss: 0.0194, Validation Loss: 0.0267
Epoch 87/100
Epoch [87/100], Training Loss: 0.0193, Validation Loss: 0.0262
Best model saved!
Epoch 88/100
Epoch [88/100], Training Loss: 0.0192, Validation Loss: 0.0267
Epoch 89/100
Epoch [89/100], Training Loss: 0.0190, Validation Loss: 0.0260
Best model saved!
Epoch 90/100
Epoch [90/100], Training Loss: 0.0190, Validation Loss: 0.0261
Epoch 91/100
Epoch [91/100], Training Loss: 0.0189, Validation Loss: 0.0263
Epoch 92/100
Epoch [92/100], Training Loss: 0.0188, Validation Loss: 0.0260
Best model saved!
Epoch 93/100
Epoch [93/100], Training Loss: 0.0187, Validation Loss: 0.0263
Epoch 94/100
Epoch [94/100], Training Loss: 0.0186, Validation Loss: 0.0262
Epoch 95/100
Epoch [95/100], Training Loss: 0.0186, Validation Loss: 0.0255
Best model saved!
Epoch 96/100
Epoch [96/100], Training Loss: 0.0181, Validation Loss: 0.0258
Epoch 97/100
Epoch [97/100], Training Loss: 0.0183, Validation Loss: 0.0253
Best model saved!
Epoch 98/100
Epoch [98/100], Training Loss: 0.0180, Validation Loss: 0.0254
Epoch 99/100
Epoch [99/100], Training Loss: 0.0178, Validation Loss: 0.0252
Best model saved!
Epoch 100/100
Epoch [100/100], Training Loss: 0.0176, Validation Loss: 0.0250
Best model saved!
Test Loss: 0.0172
done with hyperparameter tuning training
Model type: UNet6, Epochs: 100, Learning rate: 0.0001, Batch size: 64
Hyperparameter tuning prediction finished
UNet6 with 0.4 0.3 100 0.0001 64 large done at Wed Nov  6 18:22:12 CET 2024
UNet6 with 0.4 0.3 150 0.0001 64 large start at Wed Nov  6 18:22:12 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 150 learning_rate: 0.0001 batch_size: 64 model_type: UNet6
Epoch 1/150
Epoch [1/150], Training Loss: 0.0545, Validation Loss: 0.0456
Best model saved!
Epoch 2/150
Epoch [2/150], Training Loss: 0.0513, Validation Loss: 0.0455
Best model saved!
Epoch 3/150
Epoch [3/150], Training Loss: 0.0495, Validation Loss: 0.0455
Best model saved!
Epoch 4/150
Epoch [4/150], Training Loss: 0.0479, Validation Loss: 0.0455
Best model saved!
Epoch 5/150
Epoch [5/150], Training Loss: 0.0462, Validation Loss: 0.0454
Best model saved!
Epoch 6/150
Epoch [6/150], Training Loss: 0.0448, Validation Loss: 0.0454
Best model saved!
Epoch 7/150
Epoch [7/150], Training Loss: 0.0435, Validation Loss: 0.0454
Best model saved!
Epoch 8/150
Epoch [8/150], Training Loss: 0.0424, Validation Loss: 0.0453
Best model saved!
Epoch 9/150
Epoch [9/150], Training Loss: 0.0412, Validation Loss: 0.0452
Best model saved!
Epoch 10/150
Epoch [10/150], Training Loss: 0.0412, Validation Loss: 0.0451
Best model saved!
Epoch 11/150
Epoch [11/150], Training Loss: 0.0399, Validation Loss: 0.0450
Best model saved!
Epoch 12/150
Epoch [12/150], Training Loss: 0.0402, Validation Loss: 0.0450
Best model saved!
Epoch 13/150
Epoch [13/150], Training Loss: 0.0389, Validation Loss: 0.0448
Best model saved!
Epoch 14/150
Epoch [14/150], Training Loss: 0.0387, Validation Loss: 0.0447
Best model saved!
Epoch 15/150
Epoch [15/150], Training Loss: 0.0380, Validation Loss: 0.0446
Best model saved!
Epoch 16/150
Epoch [16/150], Training Loss: 0.0372, Validation Loss: 0.0444
Best model saved!
Epoch 17/150
Epoch [17/150], Training Loss: 0.0367, Validation Loss: 0.0442
Best model saved!
Epoch 18/150
Epoch [18/150], Training Loss: 0.0361, Validation Loss: 0.0439
Best model saved!
Epoch 19/150
Epoch [19/150], Training Loss: 0.0356, Validation Loss: 0.0437
Best model saved!
Epoch 20/150
Epoch [20/150], Training Loss: 0.0349, Validation Loss: 0.0435
Best model saved!
Epoch 21/150
Epoch [21/150], Training Loss: 0.0345, Validation Loss: 0.0431
Best model saved!
Epoch 22/150
Epoch [22/150], Training Loss: 0.0340, Validation Loss: 0.0427
Best model saved!
Epoch 23/150
Epoch [23/150], Training Loss: 0.0335, Validation Loss: 0.0422
Best model saved!
Epoch 24/150
UNet6 with 0.4 0.3 150 0.0001 64 large done at Wed Nov  6 21:08:13 CET 2024
UNet6 with 0.4 0.3 200 0.0001 64 large start at Wed Nov  6 21:08:13 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
corrected prediction files not there yet
no best model found
results/testing/larger_area_048/720/UNet6_200_0.0001_64/best_model.pth
UNet6 with 0.4 0.3 200 0.0001 64 large done at Wed Nov  6 21:08:18 CET 2024
SBATCH job finished
