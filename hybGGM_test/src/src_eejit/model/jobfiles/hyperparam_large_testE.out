SBATCH job
Started 07/11/2024 10:20:55
Working directory /eejit/home/hausw001/HybGGM/hybGGM_test
UNet6 with 0.4 0.3 100 0.0001 64 large start at Thu Nov  7 10:20:56 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
corrected prediction files not there yet
no best model found
results/testing/larger_area_048/600/UNet6_100_0.0001_64/best_model.pth
testSize: 0.4 trainSize: 0.3 epochs: 100 learning_rate: 0.0001 batch_size: 64 model_type: UNet6
Epoch 1/100
Epoch [1/100], Training Loss: 0.0545, Validation Loss: 0.0456
Best model saved!
Epoch 2/100
Epoch [2/100], Training Loss: 0.0513, Validation Loss: 0.0455
Best model saved!
Epoch 3/100
Epoch [3/100], Training Loss: 0.0495, Validation Loss: 0.0455
Best model saved!
Epoch 4/100
Epoch [4/100], Training Loss: 0.0479, Validation Loss: 0.0455
Best model saved!
Epoch 5/100
Epoch [5/100], Training Loss: 0.0462, Validation Loss: 0.0454
Best model saved!
Epoch 6/100
Epoch [6/100], Training Loss: 0.0448, Validation Loss: 0.0454
Best model saved!
Epoch 7/100
Epoch [7/100], Training Loss: 0.0435, Validation Loss: 0.0454
Best model saved!
Epoch 8/100
Epoch [8/100], Training Loss: 0.0424, Validation Loss: 0.0453
Best model saved!
Epoch 9/100
Epoch [9/100], Training Loss: 0.0412, Validation Loss: 0.0452
Best model saved!
Epoch 10/100
Epoch [10/100], Training Loss: 0.0412, Validation Loss: 0.0451
Best model saved!
Epoch 11/100
Epoch [11/100], Training Loss: 0.0399, Validation Loss: 0.0450
Best model saved!
Epoch 12/100
Epoch [12/100], Training Loss: 0.0402, Validation Loss: 0.0450
Best model saved!
Epoch 13/100
Epoch [13/100], Training Loss: 0.0389, Validation Loss: 0.0448
Best model saved!
Epoch 14/100
Epoch [14/100], Training Loss: 0.0387, Validation Loss: 0.0447
Best model saved!
Epoch 15/100
Epoch [15/100], Training Loss: 0.0380, Validation Loss: 0.0446
Best model saved!
Epoch 16/100
Epoch [16/100], Training Loss: 0.0372, Validation Loss: 0.0444
Best model saved!
Epoch 17/100
Epoch [17/100], Training Loss: 0.0367, Validation Loss: 0.0442
Best model saved!
Epoch 18/100
Epoch [18/100], Training Loss: 0.0361, Validation Loss: 0.0439
Best model saved!
Epoch 19/100
Epoch [19/100], Training Loss: 0.0356, Validation Loss: 0.0437
Best model saved!
Epoch 20/100
Epoch [20/100], Training Loss: 0.0349, Validation Loss: 0.0435
Best model saved!
Epoch 21/100
Epoch [21/100], Training Loss: 0.0345, Validation Loss: 0.0431
Best model saved!
Epoch 22/100
Epoch [22/100], Training Loss: 0.0340, Validation Loss: 0.0427
Best model saved!
Epoch 23/100
Epoch [23/100], Training Loss: 0.0335, Validation Loss: 0.0422
Best model saved!
Epoch 24/100
Epoch [24/100], Training Loss: 0.0336, Validation Loss: 0.0418
Best model saved!
Epoch 25/100
Epoch [25/100], Training Loss: 0.0336, Validation Loss: 0.0411
Best model saved!
Epoch 26/100
Epoch [26/100], Training Loss: 0.0344, Validation Loss: 0.0408
Best model saved!
Epoch 27/100
Epoch [27/100], Training Loss: 0.0332, Validation Loss: 0.0405
Best model saved!
Epoch 28/100
Epoch [28/100], Training Loss: 0.0329, Validation Loss: 0.0398
Best model saved!
Epoch 29/100
Epoch [29/100], Training Loss: 0.0326, Validation Loss: 0.0390
Best model saved!
Epoch 30/100
Epoch [30/100], Training Loss: 0.0322, Validation Loss: 0.0385
Best model saved!
Epoch 31/100
Epoch [31/100], Training Loss: 0.0321, Validation Loss: 0.0383
Best model saved!
Epoch 32/100
Epoch [32/100], Training Loss: 0.0317, Validation Loss: 0.0382
Best model saved!
Epoch 33/100
Epoch [33/100], Training Loss: 0.0314, Validation Loss: 0.0378
Best model saved!
Epoch 34/100
Epoch [34/100], Training Loss: 0.0310, Validation Loss: 0.0372
Best model saved!
Epoch 35/100
Epoch [35/100], Training Loss: 0.0308, Validation Loss: 0.0368
Best model saved!
Epoch 36/100
Epoch [36/100], Training Loss: 0.0305, Validation Loss: 0.0364
Best model saved!
Epoch 37/100
Epoch [37/100], Training Loss: 0.0301, Validation Loss: 0.0361
Best model saved!
Epoch 38/100
Epoch [38/100], Training Loss: 0.0299, Validation Loss: 0.0356
Best model saved!
Epoch 39/100
Epoch [39/100], Training Loss: 0.0296, Validation Loss: 0.0351
Best model saved!
Epoch 40/100
Epoch [40/100], Training Loss: 0.0293, Validation Loss: 0.0350
Best model saved!
Epoch 41/100
Epoch [41/100], Training Loss: 0.0290, Validation Loss: 0.0348
Best model saved!
Epoch 42/100
Epoch [42/100], Training Loss: 0.0287, Validation Loss: 0.0346
Best model saved!
Epoch 43/100
Epoch [43/100], Training Loss: 0.0284, Validation Loss: 0.0341
Best model saved!
Epoch 44/100
Epoch [44/100], Training Loss: 0.0282, Validation Loss: 0.0342
Epoch 45/100
Epoch [45/100], Training Loss: 0.0280, Validation Loss: 0.0333
Best model saved!
Epoch 46/100
Epoch [46/100], Training Loss: 0.0284, Validation Loss: 0.0330
Best model saved!
Epoch 47/100
Epoch [47/100], Training Loss: 0.0278, Validation Loss: 0.0333
Epoch 48/100
Epoch [48/100], Training Loss: 0.0279, Validation Loss: 0.0321
Best model saved!
Epoch 49/100
Epoch [49/100], Training Loss: 0.0274, Validation Loss: 0.0321
Epoch 50/100
Epoch [50/100], Training Loss: 0.0271, Validation Loss: 0.0316
Best model saved!
Epoch 51/100
Epoch [51/100], Training Loss: 0.0269, Validation Loss: 0.0312
Best model saved!
Epoch 52/100
Epoch [52/100], Training Loss: 0.0265, Validation Loss: 0.0310
Best model saved!
Epoch 53/100
Epoch [53/100], Training Loss: 0.0263, Validation Loss: 0.0309
Best model saved!
Epoch 54/100
Epoch [54/100], Training Loss: 0.0258, Validation Loss: 0.0308
Best model saved!
Epoch 55/100
Epoch [55/100], Training Loss: 0.0258, Validation Loss: 0.0302
Best model saved!
Epoch 56/100
Epoch [56/100], Training Loss: 0.0253, Validation Loss: 0.0302
Epoch 57/100
Epoch [57/100], Training Loss: 0.0252, Validation Loss: 0.0300
Best model saved!
Epoch 58/100
Epoch [58/100], Training Loss: 0.0248, Validation Loss: 0.0297
Best model saved!
Epoch 59/100
Epoch [59/100], Training Loss: 0.0246, Validation Loss: 0.0298
Epoch 60/100
Epoch [60/100], Training Loss: 0.0243, Validation Loss: 0.0297
Best model saved!
Epoch 61/100
Epoch [61/100], Training Loss: 0.0240, Validation Loss: 0.0296
Best model saved!
Epoch 62/100
Epoch [62/100], Training Loss: 0.0238, Validation Loss: 0.0297
Epoch 63/100
Epoch [63/100], Training Loss: 0.0237, Validation Loss: 0.0309
Epoch 64/100
Epoch [64/100], Training Loss: 0.0240, Validation Loss: 0.0306
Epoch 65/100
Epoch [65/100], Training Loss: 0.0239, Validation Loss: 0.0289
Best model saved!
Epoch 66/100
Epoch [66/100], Training Loss: 0.0232, Validation Loss: 0.0291
Epoch 67/100
Epoch [67/100], Training Loss: 0.0232, Validation Loss: 0.0285
Best model saved!
Epoch 68/100
Epoch [68/100], Training Loss: 0.0227, Validation Loss: 0.0285
Best model saved!
Epoch 69/100
Epoch [69/100], Training Loss: 0.0227, Validation Loss: 0.0276
Best model saved!
Epoch 70/100
Epoch [70/100], Training Loss: 0.0223, Validation Loss: 0.0282
Epoch 71/100
Epoch [71/100], Training Loss: 0.0222, Validation Loss: 0.0279
Epoch 72/100
Epoch [72/100], Training Loss: 0.0221, Validation Loss: 0.0283
Epoch 73/100
Epoch [73/100], Training Loss: 0.0221, Validation Loss: 0.0277
Epoch 74/100
Epoch [74/100], Training Loss: 0.0216, Validation Loss: 0.0278
Epoch 75/100
Epoch [75/100], Training Loss: 0.0215, Validation Loss: 0.0276
Epoch 76/100
Epoch [76/100], Training Loss: 0.0212, Validation Loss: 0.0277
Epoch 77/100
Epoch [77/100], Training Loss: 0.0212, Validation Loss: 0.0272
Best model saved!
Epoch 78/100
Epoch [78/100], Training Loss: 0.0209, Validation Loss: 0.0277
Epoch 79/100
Epoch [79/100], Training Loss: 0.0207, Validation Loss: 0.0272
Best model saved!
Epoch 80/100
Epoch [80/100], Training Loss: 0.0206, Validation Loss: 0.0269
Best model saved!
Epoch 81/100
Epoch [81/100], Training Loss: 0.0203, Validation Loss: 0.0266
Best model saved!
Epoch 82/100
Epoch [82/100], Training Loss: 0.0200, Validation Loss: 0.0268
Epoch 83/100
Epoch [83/100], Training Loss: 0.0199, Validation Loss: 0.0267
Epoch 84/100
Epoch [84/100], Training Loss: 0.0198, Validation Loss: 0.0266
Best model saved!
Epoch 85/100
Epoch [85/100], Training Loss: 0.0196, Validation Loss: 0.0265
Best model saved!
Epoch 86/100
Epoch [86/100], Training Loss: 0.0194, Validation Loss: 0.0267
Epoch 87/100
Epoch [87/100], Training Loss: 0.0193, Validation Loss: 0.0262
Best model saved!
Epoch 88/100
Epoch [88/100], Training Loss: 0.0192, Validation Loss: 0.0267
Epoch 89/100
Epoch [89/100], Training Loss: 0.0190, Validation Loss: 0.0260
Best model saved!
Epoch 90/100
Epoch [90/100], Training Loss: 0.0190, Validation Loss: 0.0261
Epoch 91/100
Epoch [91/100], Training Loss: 0.0189, Validation Loss: 0.0263
Epoch 92/100
Epoch [92/100], Training Loss: 0.0188, Validation Loss: 0.0260
Best model saved!
Epoch 93/100
Epoch [93/100], Training Loss: 0.0187, Validation Loss: 0.0263
Epoch 94/100
Epoch [94/100], Training Loss: 0.0186, Validation Loss: 0.0262
Epoch 95/100
Epoch [95/100], Training Loss: 0.0186, Validation Loss: 0.0255
Best model saved!
Epoch 96/100
Epoch [96/100], Training Loss: 0.0181, Validation Loss: 0.0258
Epoch 97/100
Epoch [97/100], Training Loss: 0.0183, Validation Loss: 0.0253
Best model saved!
Epoch 98/100
Epoch [98/100], Training Loss: 0.0180, Validation Loss: 0.0254
Epoch 99/100
Epoch [99/100], Training Loss: 0.0178, Validation Loss: 0.0252
Best model saved!
Epoch 100/100
Epoch [100/100], Training Loss: 0.0176, Validation Loss: 0.0250
Best model saved!
Test Loss: 0.0172
done with hyperparameter tuning training
load validation data
load model
run prediction
load out_var
save results/testing/larger_area_048/600/UNet6_100_0.0001_64/y_pred_denorm_new.npy
Hyperparameter tuning prediction finished
UNet6 with 0.4 0.3 100 0.0001 64 large done at Thu Nov  7 22:05:25 CET 2024
UNet6 with 0.4 0.3 100 0.001 64 large start at Thu Nov  7 22:05:25 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
corrected prediction files not there yet
no best model found
results/testing/larger_area_048/600/UNet6_100_0.001_64/best_model.pth
testSize: 0.4 trainSize: 0.3 epochs: 100 learning_rate: 0.001 batch_size: 64 model_type: UNet6
Epoch 1/100
Epoch [1/100], Training Loss: 0.0545, Validation Loss: 0.0455
Best model saved!
Epoch 2/100
Epoch [2/100], Training Loss: 0.0598, Validation Loss: 0.0455
Best model saved!
Epoch 3/100
Epoch [3/100], Training Loss: 0.0481, Validation Loss: 0.0608
Epoch 4/100
Epoch [4/100], Training Loss: 0.0476, Validation Loss: 1.6913
Epoch 5/100
Epoch [5/100], Training Loss: 0.0439, Validation Loss: 7.2735
Epoch 6/100
Epoch [6/100], Training Loss: 0.0423, Validation Loss: 11.0391
Epoch 7/100
Epoch [7/100], Training Loss: 0.0423, Validation Loss: 10.3281
Epoch 8/100
Epoch [8/100], Training Loss: 0.0414, Validation Loss: 5.3838
Epoch 9/100
Epoch [9/100], Training Loss: 0.0408, Validation Loss: 2.0709
Epoch 10/100
Epoch [10/100], Training Loss: 0.0405, Validation Loss: 0.9781
Epoch 11/100
Epoch [11/100], Training Loss: 0.0401, Validation Loss: 0.8843
Epoch 12/100
Epoch [12/100], Training Loss: 0.0394, Validation Loss: 1.5238
Epoch 13/100
Epoch [13/100], Training Loss: 0.0391, Validation Loss: 1.7411
Epoch 14/100
Epoch [14/100], Training Loss: 0.0387, Validation Loss: 1.3856
Epoch 15/100
Epoch [15/100], Training Loss: 0.0383, Validation Loss: 0.8403
Epoch 16/100
Epoch [16/100], Training Loss: 0.0385, Validation Loss: 0.2367
Epoch 17/100
Epoch [17/100], Training Loss: 0.0389, Validation Loss: 0.1172
Epoch 18/100
Epoch [18/100], Training Loss: 0.0378, Validation Loss: 0.0866
Epoch 19/100
Epoch [19/100], Training Loss: 0.0386, Validation Loss: 0.0503
Epoch 20/100
Epoch [20/100], Training Loss: 0.0374, Validation Loss: 0.0485
Epoch 21/100
Epoch [21/100], Training Loss: 0.0376, Validation Loss: 0.0462
Epoch 22/100
Epoch [22/100], Training Loss: 0.0370, Validation Loss: 0.0436
Best model saved!
Epoch 23/100
Epoch [23/100], Training Loss: 0.0373, Validation Loss: 0.0416
Best model saved!
Epoch 24/100
Epoch [24/100], Training Loss: 0.0365, Validation Loss: 0.0411
Best model saved!
Epoch 25/100
Epoch [25/100], Training Loss: 0.0365, Validation Loss: 0.0394
Best model saved!
Epoch 26/100
Epoch [26/100], Training Loss: 0.0361, Validation Loss: 0.0377
Best model saved!
Epoch 27/100
Epoch [27/100], Training Loss: 0.0361, Validation Loss: 0.0377
Epoch 28/100
Epoch [28/100], Training Loss: 0.0359, Validation Loss: 0.0396
Epoch 29/100
Epoch [29/100], Training Loss: 0.0355, Validation Loss: 0.0429
Epoch 30/100
Epoch [30/100], Training Loss: 0.0354, Validation Loss: 0.0471
Epoch 31/100
Epoch [31/100], Training Loss: 0.0356, Validation Loss: 0.0433
Epoch 32/100
Epoch [32/100], Training Loss: 0.0350, Validation Loss: 0.0415
Epoch 33/100
Epoch [33/100], Training Loss: 0.0346, Validation Loss: 0.0527
Epoch 34/100
Epoch [34/100], Training Loss: 0.0343, Validation Loss: 0.0588
Epoch 35/100
Epoch [35/100], Training Loss: 0.0347, Validation Loss: 0.0384
Epoch 36/100
Epoch [36/100], Training Loss: 0.0367, Validation Loss: 0.0392
Epoch 37/100
Epoch [37/100], Training Loss: 0.0352, Validation Loss: 0.0373
Best model saved!
Epoch 38/100
Epoch [38/100], Training Loss: 0.0355, Validation Loss: 0.0399
Epoch 39/100
Epoch [39/100], Training Loss: 0.0346, Validation Loss: 0.0413
Epoch 40/100
Epoch [40/100], Training Loss: 0.0350, Validation Loss: 0.0383
Epoch 41/100
Epoch [41/100], Training Loss: 0.0344, Validation Loss: 0.0375
Epoch 42/100
Epoch [42/100], Training Loss: 0.0342, Validation Loss: 0.0373
Best model saved!
Epoch 43/100
Epoch [43/100], Training Loss: 0.0341, Validation Loss: 0.0373
Epoch 44/100
Epoch [44/100], Training Loss: 0.0337, Validation Loss: 0.0375
Epoch 45/100
Epoch [45/100], Training Loss: 0.0337, Validation Loss: 0.0363
Best model saved!
Epoch 46/100
Epoch [46/100], Training Loss: 0.0336, Validation Loss: 0.0363
Best model saved!
Epoch 47/100
Epoch [47/100], Training Loss: 0.0331, Validation Loss: 0.0370
Epoch 48/100
Epoch [48/100], Training Loss: 0.0333, Validation Loss: 0.0364
Epoch 49/100
Epoch [49/100], Training Loss: 0.0330, Validation Loss: 0.0366
Epoch 50/100
Epoch [50/100], Training Loss: 0.0328, Validation Loss: 0.0373
Epoch 51/100
Epoch [51/100], Training Loss: 0.0328, Validation Loss: 0.0382
Epoch 52/100
Epoch [52/100], Training Loss: 0.0323, Validation Loss: 0.0432
Epoch 53/100
Epoch [53/100], Training Loss: 0.0323, Validation Loss: 0.0398
Epoch 54/100
Epoch [54/100], Training Loss: 0.0321, Validation Loss: 0.0396
Epoch 55/100
Epoch [55/100], Training Loss: 0.0320, Validation Loss: 0.0400
Epoch 56/100
Epoch [56/100], Training Loss: 0.0316, Validation Loss: 0.0430
Epoch 57/100
Epoch [57/100], Training Loss: 0.0316, Validation Loss: 0.0455
Epoch 58/100
Epoch [58/100], Training Loss: 0.0313, Validation Loss: 0.0463
Epoch 59/100
Epoch [59/100], Training Loss: 0.0314, Validation Loss: 0.0442
Epoch 60/100
Epoch [60/100], Training Loss: 0.0322, Validation Loss: 0.0434
Epoch 61/100
Epoch [61/100], Training Loss: 0.0331, Validation Loss: 0.0415
Epoch 62/100
Epoch [62/100], Training Loss: 0.0325, Validation Loss: 0.0393
Epoch 63/100
Epoch [63/100], Training Loss: 0.0321, Validation Loss: 0.0412
Epoch 64/100
Epoch [64/100], Training Loss: 0.0323, Validation Loss: 0.0426
Epoch 65/100
Epoch [65/100], Training Loss: 0.0315, Validation Loss: 0.0416
Epoch 66/100
Epoch [66/100], Training Loss: 0.0316, Validation Loss: 0.0418
Epoch 67/100
Epoch [67/100], Training Loss: 0.0311, Validation Loss: 0.0402
Epoch 68/100
Epoch [68/100], Training Loss: 0.0310, Validation Loss: 0.0374
Epoch 69/100
Epoch [69/100], Training Loss: 0.0311, Validation Loss: 0.0377
Epoch 70/100
Epoch [70/100], Training Loss: 0.0308, Validation Loss: 0.0391
Epoch 71/100
Epoch [71/100], Training Loss: 0.0306, Validation Loss: 0.0373
Epoch 72/100
Epoch [72/100], Training Loss: 0.0303, Validation Loss: 0.0360
Best model saved!
Epoch 73/100
Epoch [73/100], Training Loss: 0.0302, Validation Loss: 0.0358
Best model saved!
Epoch 74/100
Epoch [74/100], Training Loss: 0.0300, Validation Loss: 0.0340
Best model saved!
Epoch 75/100
Epoch [75/100], Training Loss: 0.0300, Validation Loss: 0.0368
Epoch 76/100
Epoch [76/100], Training Loss: 0.0299, Validation Loss: 0.0346
Epoch 77/100
Epoch [77/100], Training Loss: 0.0303, Validation Loss: 0.0346
Epoch 78/100
Epoch [78/100], Training Loss: 0.0304, Validation Loss: 0.0363
Epoch 79/100
Epoch [79/100], Training Loss: 0.0305, Validation Loss: 0.0383
Epoch 80/100
Epoch [80/100], Training Loss: 0.0324, Validation Loss: 0.0355
Epoch 81/100
Epoch [81/100], Training Loss: 0.0306, Validation Loss: 0.0351
Epoch 82/100
Epoch [82/100], Training Loss: 0.0314, Validation Loss: 0.0339
Best model saved!
Epoch 83/100
Epoch [83/100], Training Loss: 0.0303, Validation Loss: 0.0357
Epoch 84/100
Epoch [84/100], Training Loss: 0.0305, Validation Loss: 0.0340
Epoch 85/100
Epoch [85/100], Training Loss: 0.0304, Validation Loss: 0.0377
Epoch 86/100
Epoch [86/100], Training Loss: 0.0310, Validation Loss: 0.0341
Epoch 87/100
Epoch [87/100], Training Loss: 0.0312, Validation Loss: 0.0348
Epoch 88/100
Epoch [88/100], Training Loss: 0.0308, Validation Loss: 0.0346
Epoch 89/100
Epoch [89/100], Training Loss: 0.0306, Validation Loss: 0.0331
Best model saved!
Epoch 90/100
Epoch [90/100], Training Loss: 0.0299, Validation Loss: 0.0328
Best model saved!
Epoch 91/100
Epoch [91/100], Training Loss: 0.0303, Validation Loss: 0.0325
Best model saved!
Epoch 92/100
Epoch [92/100], Training Loss: 0.0295, Validation Loss: 0.0330
Epoch 93/100
Epoch [93/100], Training Loss: 0.0295, Validation Loss: 0.0321
Best model saved!
Epoch 94/100
Epoch [94/100], Training Loss: 0.0293, Validation Loss: 0.0317
Best model saved!
Epoch 95/100
Epoch [95/100], Training Loss: 0.0295, Validation Loss: 0.0314
Best model saved!
Epoch 96/100
Epoch [96/100], Training Loss: 0.0289, Validation Loss: 0.0311
Best model saved!
Epoch 97/100
Epoch [97/100], Training Loss: 0.0288, Validation Loss: 0.0311
Epoch 98/100
Epoch [98/100], Training Loss: 0.0286, Validation Loss: 0.0312
Epoch 99/100
Epoch [99/100], Training Loss: 0.0285, Validation Loss: 0.0317
Epoch 100/100
Epoch [100/100], Training Loss: 0.0283, Validation Loss: 0.0314
Test Loss: 0.0214
done with hyperparameter tuning training
load validation data
load model
run prediction
load out_var
save results/testing/larger_area_048/600/UNet6_100_0.001_64/y_pred_denorm_new.npy
Hyperparameter tuning prediction finished
UNet6 with 0.4 0.3 100 0.001 64 large done at Fri Nov  8 10:00:50 CET 2024
UNet6 with 0.4 0.3 150 0.0001 64 large start at Fri Nov  8 10:00:50 CET 2024
