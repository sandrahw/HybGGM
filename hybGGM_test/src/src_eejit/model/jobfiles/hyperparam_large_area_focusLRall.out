SBATCH job
Started 05/11/2024 23:17:50
Working directory /eejit/home/hausw001/HybGGM/hybGGM_test
UNet6 with 0.4 0.3 50 1e-05 64 large start at Tue Nov  5 23:17:52 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 50 learning_rate: 1e-05 batch_size: 64 model_type: UNet6
Epoch 1/50
Epoch [1/50], Training Loss: 0.0545, Validation Loss: 0.0456
Best model saved!
Epoch 2/50
Epoch [2/50], Training Loss: 0.0540, Validation Loss: 0.0456
Epoch 3/50
Epoch [3/50], Training Loss: 0.0536, Validation Loss: 0.0456
Epoch 4/50
Epoch [4/50], Training Loss: 0.0532, Validation Loss: 0.0456
Epoch 5/50
Epoch [5/50], Training Loss: 0.0528, Validation Loss: 0.0456
Epoch 6/50
Epoch [6/50], Training Loss: 0.0524, Validation Loss: 0.0456
Epoch 7/50
Epoch [7/50], Training Loss: 0.0521, Validation Loss: 0.0456
Epoch 8/50
Epoch [8/50], Training Loss: 0.0518, Validation Loss: 0.0456
Epoch 9/50
Epoch [9/50], Training Loss: 0.0515, Validation Loss: 0.0457
Epoch 10/50
Epoch [10/50], Training Loss: 0.0512, Validation Loss: 0.0457
Epoch 11/50
Epoch [11/50], Training Loss: 0.0509, Validation Loss: 0.0457
Epoch 12/50
Epoch [12/50], Training Loss: 0.0506, Validation Loss: 0.0457
Epoch 13/50
Epoch [13/50], Training Loss: 0.0504, Validation Loss: 0.0457
Epoch 14/50
Epoch [14/50], Training Loss: 0.0501, Validation Loss: 0.0458
Epoch 15/50
Epoch [15/50], Training Loss: 0.0499, Validation Loss: 0.0458
Epoch 16/50
Epoch [16/50], Training Loss: 0.0496, Validation Loss: 0.0458
Epoch 17/50
Epoch [17/50], Training Loss: 0.0493, Validation Loss: 0.0458
Epoch 18/50
Epoch [18/50], Training Loss: 0.0491, Validation Loss: 0.0458
Epoch 19/50
Epoch [19/50], Training Loss: 0.0488, Validation Loss: 0.0458
Epoch 20/50
Epoch [20/50], Training Loss: 0.0485, Validation Loss: 0.0458
Epoch 21/50
Epoch [21/50], Training Loss: 0.0483, Validation Loss: 0.0458
Epoch 22/50
Epoch [22/50], Training Loss: 0.0480, Validation Loss: 0.0458
Epoch 23/50
Epoch [23/50], Training Loss: 0.0477, Validation Loss: 0.0457
Epoch 24/50
Epoch [24/50], Training Loss: 0.0474, Validation Loss: 0.0457
Epoch 25/50
Epoch [25/50], Training Loss: 0.0471, Validation Loss: 0.0456
Epoch 26/50
Epoch [26/50], Training Loss: 0.0468, Validation Loss: 0.0456
Best model saved!
Epoch 27/50
Epoch [27/50], Training Loss: 0.0465, Validation Loss: 0.0455
Best model saved!
Epoch 28/50
Epoch [28/50], Training Loss: 0.0462, Validation Loss: 0.0454
Best model saved!
Epoch 29/50
Epoch [29/50], Training Loss: 0.0459, Validation Loss: 0.0453
Best model saved!
Epoch 30/50
Epoch [30/50], Training Loss: 0.0456, Validation Loss: 0.0451
Best model saved!
Epoch 31/50
Epoch [31/50], Training Loss: 0.0453, Validation Loss: 0.0450
Best model saved!
Epoch 32/50
Epoch [32/50], Training Loss: 0.0450, Validation Loss: 0.0448
Best model saved!
Epoch 33/50
Epoch [33/50], Training Loss: 0.0447, Validation Loss: 0.0447
Best model saved!
Epoch 34/50
Epoch [34/50], Training Loss: 0.0444, Validation Loss: 0.0445
Best model saved!
Epoch 35/50
Epoch [35/50], Training Loss: 0.0440, Validation Loss: 0.0443
Best model saved!
Epoch 36/50
Epoch [36/50], Training Loss: 0.0437, Validation Loss: 0.0441
Best model saved!
Epoch 37/50
Epoch [37/50], Training Loss: 0.0433, Validation Loss: 0.0438
Best model saved!
Epoch 38/50
Epoch [38/50], Training Loss: 0.0430, Validation Loss: 0.0436
Best model saved!
Epoch 39/50
Epoch [39/50], Training Loss: 0.0426, Validation Loss: 0.0433
Best model saved!
Epoch 40/50
Epoch [40/50], Training Loss: 0.0422, Validation Loss: 0.0431
Best model saved!
Epoch 41/50
Epoch [41/50], Training Loss: 0.0418, Validation Loss: 0.0425
Best model saved!
Epoch 42/50
Epoch [42/50], Training Loss: 0.0415, Validation Loss: 0.0426
Epoch 43/50
Epoch [43/50], Training Loss: 0.0411, Validation Loss: 0.0424
Best model saved!
Epoch 44/50
Epoch [44/50], Training Loss: 0.0407, Validation Loss: 0.0417
Best model saved!
Epoch 45/50
Epoch [45/50], Training Loss: 0.0404, Validation Loss: 0.0416
Best model saved!
Epoch 46/50
Epoch [46/50], Training Loss: 0.0400, Validation Loss: 0.0417
Epoch 47/50
Epoch [47/50], Training Loss: 0.0396, Validation Loss: 0.0413
Best model saved!
Epoch 48/50
Epoch [48/50], Training Loss: 0.0392, Validation Loss: 0.0408
Best model saved!
Epoch 49/50
Epoch [49/50], Training Loss: 0.0389, Validation Loss: 0.0407
Best model saved!
Epoch 50/50
Epoch [50/50], Training Loss: 0.0385, Validation Loss: 0.0405
Best model saved!
Test Loss: 0.0290
done with hyperparameter tuning training
Model type: UNet6, Epochs: 50, Learning rate: 1e-05, Batch size: 64
Hyperparameter tuning prediction finished
UNet6 with 0.4 0.3 50 1e-05 64 large done at Wed Nov  6 05:06:24 CET 2024
UNet6 with 0.4 0.3 50 0.0001 64 large start at Wed Nov  6 05:06:24 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 50 learning_rate: 0.0001 batch_size: 64 model_type: UNet6
Epoch 1/50
Epoch [1/50], Training Loss: 0.0545, Validation Loss: 0.0456
Best model saved!
Epoch 2/50
Epoch [2/50], Training Loss: 0.0513, Validation Loss: 0.0455
Best model saved!
Epoch 3/50
Epoch [3/50], Training Loss: 0.0495, Validation Loss: 0.0455
Best model saved!
Epoch 4/50
Epoch [4/50], Training Loss: 0.0479, Validation Loss: 0.0455
Best model saved!
Epoch 5/50
Epoch [5/50], Training Loss: 0.0462, Validation Loss: 0.0454
Best model saved!
Epoch 6/50
Epoch [6/50], Training Loss: 0.0448, Validation Loss: 0.0454
Best model saved!
Epoch 7/50
Epoch [7/50], Training Loss: 0.0435, Validation Loss: 0.0454
Best model saved!
Epoch 8/50
Epoch [8/50], Training Loss: 0.0424, Validation Loss: 0.0453
Best model saved!
Epoch 9/50
Epoch [9/50], Training Loss: 0.0412, Validation Loss: 0.0452
Best model saved!
Epoch 10/50
Epoch [10/50], Training Loss: 0.0412, Validation Loss: 0.0451
Best model saved!
Epoch 11/50
Epoch [11/50], Training Loss: 0.0399, Validation Loss: 0.0450
Best model saved!
Epoch 12/50
Epoch [12/50], Training Loss: 0.0402, Validation Loss: 0.0450
Best model saved!
Epoch 13/50
Epoch [13/50], Training Loss: 0.0389, Validation Loss: 0.0448
Best model saved!
Epoch 14/50
Epoch [14/50], Training Loss: 0.0387, Validation Loss: 0.0447
Best model saved!
Epoch 15/50
Epoch [15/50], Training Loss: 0.0380, Validation Loss: 0.0446
Best model saved!
Epoch 16/50
Epoch [16/50], Training Loss: 0.0372, Validation Loss: 0.0444
Best model saved!
Epoch 17/50
Epoch [17/50], Training Loss: 0.0367, Validation Loss: 0.0442
Best model saved!
Epoch 18/50
Epoch [18/50], Training Loss: 0.0361, Validation Loss: 0.0439
Best model saved!
Epoch 19/50
Epoch [19/50], Training Loss: 0.0356, Validation Loss: 0.0437
Best model saved!
Epoch 20/50
Epoch [20/50], Training Loss: 0.0349, Validation Loss: 0.0435
Best model saved!
Epoch 21/50
Epoch [21/50], Training Loss: 0.0345, Validation Loss: 0.0431
Best model saved!
Epoch 22/50
Epoch [22/50], Training Loss: 0.0340, Validation Loss: 0.0427
Best model saved!
Epoch 23/50
Epoch [23/50], Training Loss: 0.0335, Validation Loss: 0.0422
Best model saved!
Epoch 24/50
Epoch [24/50], Training Loss: 0.0336, Validation Loss: 0.0418
Best model saved!
Epoch 25/50
Epoch [25/50], Training Loss: 0.0336, Validation Loss: 0.0411
Best model saved!
Epoch 26/50
Epoch [26/50], Training Loss: 0.0344, Validation Loss: 0.0408
Best model saved!
Epoch 27/50
Epoch [27/50], Training Loss: 0.0332, Validation Loss: 0.0405
Best model saved!
Epoch 28/50
Epoch [28/50], Training Loss: 0.0329, Validation Loss: 0.0398
Best model saved!
Epoch 29/50
Epoch [29/50], Training Loss: 0.0326, Validation Loss: 0.0390
Best model saved!
Epoch 30/50
Epoch [30/50], Training Loss: 0.0322, Validation Loss: 0.0385
Best model saved!
Epoch 31/50
Epoch [31/50], Training Loss: 0.0321, Validation Loss: 0.0383
Best model saved!
Epoch 32/50
Epoch [32/50], Training Loss: 0.0317, Validation Loss: 0.0382
Best model saved!
Epoch 33/50
Epoch [33/50], Training Loss: 0.0314, Validation Loss: 0.0378
Best model saved!
Epoch 34/50
Epoch [34/50], Training Loss: 0.0310, Validation Loss: 0.0372
Best model saved!
Epoch 35/50
Epoch [35/50], Training Loss: 0.0308, Validation Loss: 0.0368
Best model saved!
Epoch 36/50
Epoch [36/50], Training Loss: 0.0305, Validation Loss: 0.0364
Best model saved!
Epoch 37/50
Epoch [37/50], Training Loss: 0.0301, Validation Loss: 0.0361
Best model saved!
Epoch 38/50
Epoch [38/50], Training Loss: 0.0299, Validation Loss: 0.0356
Best model saved!
Epoch 39/50
Epoch [39/50], Training Loss: 0.0296, Validation Loss: 0.0351
Best model saved!
Epoch 40/50
Epoch [40/50], Training Loss: 0.0293, Validation Loss: 0.0350
Best model saved!
Epoch 41/50
Epoch [41/50], Training Loss: 0.0290, Validation Loss: 0.0348
Best model saved!
Epoch 42/50
Epoch [42/50], Training Loss: 0.0287, Validation Loss: 0.0346
Best model saved!
Epoch 43/50
Epoch [43/50], Training Loss: 0.0284, Validation Loss: 0.0341
Best model saved!
Epoch 44/50
Epoch [44/50], Training Loss: 0.0282, Validation Loss: 0.0342
Epoch 45/50
Epoch [45/50], Training Loss: 0.0280, Validation Loss: 0.0333
Best model saved!
Epoch 46/50
Epoch [46/50], Training Loss: 0.0284, Validation Loss: 0.0330
Best model saved!
Epoch 47/50
Epoch [47/50], Training Loss: 0.0278, Validation Loss: 0.0333
Epoch 48/50
Epoch [48/50], Training Loss: 0.0279, Validation Loss: 0.0321
Best model saved!
Epoch 49/50
Epoch [49/50], Training Loss: 0.0274, Validation Loss: 0.0321
Epoch 50/50
Epoch [50/50], Training Loss: 0.0271, Validation Loss: 0.0316
Best model saved!
Test Loss: 0.0221
done with hyperparameter tuning training
Model type: UNet6, Epochs: 50, Learning rate: 0.0001, Batch size: 64
Hyperparameter tuning prediction finished
UNet6 with 0.4 0.3 50 0.0001 64 large done at Wed Nov  6 11:05:59 CET 2024
UNet6 with 0.4 0.3 50 0.001 64 large start at Wed Nov  6 11:05:59 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 50 learning_rate: 0.001 batch_size: 64 model_type: UNet6
Epoch 1/50
Epoch [1/50], Training Loss: 0.0545, Validation Loss: 0.0455
Best model saved!
Epoch 2/50
Epoch [2/50], Training Loss: 0.0598, Validation Loss: 0.0455
Best model saved!
Epoch 3/50
Epoch [3/50], Training Loss: 0.0481, Validation Loss: 0.0608
Epoch 4/50
Epoch [4/50], Training Loss: 0.0476, Validation Loss: 1.6913
Epoch 5/50
Epoch [5/50], Training Loss: 0.0439, Validation Loss: 7.2735
Epoch 6/50
Epoch [6/50], Training Loss: 0.0423, Validation Loss: 11.0391
Epoch 7/50
Epoch [7/50], Training Loss: 0.0423, Validation Loss: 10.3281
Epoch 8/50
Epoch [8/50], Training Loss: 0.0414, Validation Loss: 5.3838
Epoch 9/50
Epoch [9/50], Training Loss: 0.0408, Validation Loss: 2.0709
Epoch 10/50
Epoch [10/50], Training Loss: 0.0405, Validation Loss: 0.9781
Epoch 11/50
Epoch [11/50], Training Loss: 0.0401, Validation Loss: 0.8843
Epoch 12/50
Epoch [12/50], Training Loss: 0.0394, Validation Loss: 1.5238
Epoch 13/50
Epoch [13/50], Training Loss: 0.0391, Validation Loss: 1.7411
Epoch 14/50
Epoch [14/50], Training Loss: 0.0387, Validation Loss: 1.3856
Epoch 15/50
Epoch [15/50], Training Loss: 0.0383, Validation Loss: 0.8403
Epoch 16/50
Epoch [16/50], Training Loss: 0.0385, Validation Loss: 0.2367
Epoch 17/50
Epoch [17/50], Training Loss: 0.0389, Validation Loss: 0.1172
Epoch 18/50
Epoch [18/50], Training Loss: 0.0378, Validation Loss: 0.0866
Epoch 19/50
Epoch [19/50], Training Loss: 0.0386, Validation Loss: 0.0503
Epoch 20/50
Epoch [20/50], Training Loss: 0.0374, Validation Loss: 0.0485
Epoch 21/50
Epoch [21/50], Training Loss: 0.0376, Validation Loss: 0.0462
Epoch 22/50
Epoch [22/50], Training Loss: 0.0370, Validation Loss: 0.0436
Best model saved!
Epoch 23/50
Epoch [23/50], Training Loss: 0.0373, Validation Loss: 0.0416
Best model saved!
Epoch 24/50
Epoch [24/50], Training Loss: 0.0365, Validation Loss: 0.0411
Best model saved!
Epoch 25/50
Epoch [25/50], Training Loss: 0.0365, Validation Loss: 0.0394
Best model saved!
Epoch 26/50
Epoch [26/50], Training Loss: 0.0361, Validation Loss: 0.0377
Best model saved!
Epoch 27/50
Epoch [27/50], Training Loss: 0.0361, Validation Loss: 0.0377
Epoch 28/50
Epoch [28/50], Training Loss: 0.0359, Validation Loss: 0.0396
Epoch 29/50
Epoch [29/50], Training Loss: 0.0355, Validation Loss: 0.0429
Epoch 30/50
Epoch [30/50], Training Loss: 0.0354, Validation Loss: 0.0471
Epoch 31/50
Epoch [31/50], Training Loss: 0.0356, Validation Loss: 0.0433
Epoch 32/50
Epoch [32/50], Training Loss: 0.0350, Validation Loss: 0.0415
Epoch 33/50
Epoch [33/50], Training Loss: 0.0346, Validation Loss: 0.0527
Epoch 34/50
Epoch [34/50], Training Loss: 0.0343, Validation Loss: 0.0588
Epoch 35/50
Epoch [35/50], Training Loss: 0.0347, Validation Loss: 0.0384
Epoch 36/50
Epoch [36/50], Training Loss: 0.0367, Validation Loss: 0.0392
Epoch 37/50
Epoch [37/50], Training Loss: 0.0352, Validation Loss: 0.0373
Best model saved!
Epoch 38/50
Epoch [38/50], Training Loss: 0.0355, Validation Loss: 0.0399
Epoch 39/50
Epoch [39/50], Training Loss: 0.0346, Validation Loss: 0.0413
Epoch 40/50
Epoch [40/50], Training Loss: 0.0350, Validation Loss: 0.0383
Epoch 41/50
Epoch [41/50], Training Loss: 0.0344, Validation Loss: 0.0375
Epoch 42/50
Epoch [42/50], Training Loss: 0.0342, Validation Loss: 0.0373
Best model saved!
Epoch 43/50
Epoch [43/50], Training Loss: 0.0341, Validation Loss: 0.0373
Epoch 44/50
Epoch [44/50], Training Loss: 0.0337, Validation Loss: 0.0375
Epoch 45/50
Epoch [45/50], Training Loss: 0.0337, Validation Loss: 0.0363
Best model saved!
Epoch 46/50
Epoch [46/50], Training Loss: 0.0336, Validation Loss: 0.0363
Best model saved!
Epoch 47/50
Epoch [47/50], Training Loss: 0.0331, Validation Loss: 0.0370
Epoch 48/50
Epoch [48/50], Training Loss: 0.0333, Validation Loss: 0.0364
Epoch 49/50
Epoch [49/50], Training Loss: 0.0330, Validation Loss: 0.0366
Epoch 50/50
Epoch [50/50], Training Loss: 0.0328, Validation Loss: 0.0373
Test Loss: 0.0264
done with hyperparameter tuning training
Model type: UNet6, Epochs: 50, Learning rate: 0.001, Batch size: 64
Hyperparameter tuning prediction finished
UNet6 with 0.4 0.3 50 0.001 64 large done at Wed Nov  6 16:52:05 CET 2024
UNet6 with 0.4 0.3 50 0.01 64 large start at Wed Nov  6 16:52:05 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 50 learning_rate: 0.01 batch_size: 64 model_type: UNet6
UNet6 with 0.4 0.3 50 0.01 64 large done at Wed Nov  6 16:53:04 CET 2024
SBATCH job finished
