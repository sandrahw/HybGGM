SBATCH job
Started 01/11/2024 16:41:39
Working directory /eejit/home/hausw001/HybGGM/hybGGM_test
UNet6 with 0.4 0.3 10 0.0001 64 small start at Fri Nov  1 16:41:40 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 10 learning_rate: 0.0001 batch_size: 64 model_type: UNet6
Epoch 1/10
Epoch [1/10], Training Loss: 0.0500, Validation Loss: 0.0523
Best model saved!
Epoch 2/10
Epoch [2/10], Training Loss: 0.0471, Validation Loss: 0.0523
Best model saved!
Epoch 3/10
Epoch [3/10], Training Loss: 0.0454, Validation Loss: 0.0522
Best model saved!
Epoch 4/10
Epoch [4/10], Training Loss: 0.0440, Validation Loss: 0.0522
Best model saved!
Epoch 5/10
Epoch [5/10], Training Loss: 0.0429, Validation Loss: 0.0521
Best model saved!
Epoch 6/10
Epoch [6/10], Training Loss: 0.0419, Validation Loss: 0.0521
Best model saved!
Epoch 7/10
Epoch [7/10], Training Loss: 0.0410, Validation Loss: 0.0520
Best model saved!
Epoch 8/10
Epoch [8/10], Training Loss: 0.0399, Validation Loss: 0.0519
Best model saved!
Epoch 9/10
Epoch [9/10], Training Loss: 0.0385, Validation Loss: 0.0518
Best model saved!
Epoch 10/10
Epoch [10/10], Training Loss: 0.0373, Validation Loss: 0.0517
Best model saved!
Test Loss: 0.0327
done with hyperparameter tuning training
Model type: UNet6, Epochs: 10, Learning rate: 0.0001, Batch size: 64
Hyperparameter tuning prediction finished
UNet6 with 0.4 0.3 10 0.0001 64 small done at Fri Nov  1 16:58:46 CET 2024
UNet6 with 0.4 0.3 50 0.0001 64 small start at Fri Nov  1 16:58:46 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 50 learning_rate: 0.0001 batch_size: 64 model_type: UNet6
Hyperparameter tuning already done and prediction files are there
UNet6 with 0.4 0.3 50 0.0001 64 small done at Fri Nov  1 16:58:51 CET 2024
UNet6 with 0.4 0.3 100 0.0001 64 small start at Fri Nov  1 16:58:51 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 100 learning_rate: 0.0001 batch_size: 64 model_type: UNet6
Epoch 1/100
Epoch [1/100], Training Loss: 0.0500, Validation Loss: 0.0523
Best model saved!
Epoch 2/100
Epoch [2/100], Training Loss: 0.0471, Validation Loss: 0.0523
Best model saved!
Epoch 3/100
Epoch [3/100], Training Loss: 0.0454, Validation Loss: 0.0522
Best model saved!
Epoch 4/100
Epoch [4/100], Training Loss: 0.0440, Validation Loss: 0.0522
Best model saved!
Epoch 5/100
Epoch [5/100], Training Loss: 0.0429, Validation Loss: 0.0521
Best model saved!
Epoch 6/100
Epoch [6/100], Training Loss: 0.0419, Validation Loss: 0.0521
Best model saved!
Epoch 7/100
Epoch [7/100], Training Loss: 0.0410, Validation Loss: 0.0520
Best model saved!
Epoch 8/100
Epoch [8/100], Training Loss: 0.0399, Validation Loss: 0.0519
Best model saved!
Epoch 9/100
Epoch [9/100], Training Loss: 0.0385, Validation Loss: 0.0518
Best model saved!
Epoch 10/100
Epoch [10/100], Training Loss: 0.0373, Validation Loss: 0.0517
Best model saved!
Epoch 11/100
Epoch [11/100], Training Loss: 0.0384, Validation Loss: 0.0516
Best model saved!
Epoch 12/100
Epoch [12/100], Training Loss: 0.0358, Validation Loss: 0.0515
Best model saved!
Epoch 13/100
Epoch [13/100], Training Loss: 0.0352, Validation Loss: 0.0514
Best model saved!
Epoch 14/100
Epoch [14/100], Training Loss: 0.0339, Validation Loss: 0.0512
Best model saved!
Epoch 15/100
Epoch [15/100], Training Loss: 0.0334, Validation Loss: 0.0511
Best model saved!
Epoch 16/100
Epoch [16/100], Training Loss: 0.0325, Validation Loss: 0.0509
Best model saved!
Epoch 17/100
Epoch [17/100], Training Loss: 0.0317, Validation Loss: 0.0507
Best model saved!
Epoch 18/100
Epoch [18/100], Training Loss: 0.0313, Validation Loss: 0.0505
Best model saved!
Epoch 19/100
Epoch [19/100], Training Loss: 0.0305, Validation Loss: 0.0502
Best model saved!
Epoch 20/100
Epoch [20/100], Training Loss: 0.0300, Validation Loss: 0.0499
Best model saved!
Epoch 21/100
Epoch [21/100], Training Loss: 0.0296, Validation Loss: 0.0496
Best model saved!
Epoch 22/100
Epoch [22/100], Training Loss: 0.0290, Validation Loss: 0.0494
Best model saved!
Epoch 23/100
Epoch [23/100], Training Loss: 0.0285, Validation Loss: 0.0489
Best model saved!
Epoch 24/100
Epoch [24/100], Training Loss: 0.0279, Validation Loss: 0.0486
Best model saved!
Epoch 25/100
Epoch [25/100], Training Loss: 0.0275, Validation Loss: 0.0482
Best model saved!
Epoch 26/100
Epoch [26/100], Training Loss: 0.0271, Validation Loss: 0.0476
Best model saved!
Epoch 27/100
Epoch [27/100], Training Loss: 0.0267, Validation Loss: 0.0473
Best model saved!
Epoch 28/100
Epoch [28/100], Training Loss: 0.0264, Validation Loss: 0.0466
Best model saved!
Epoch 29/100
Epoch [29/100], Training Loss: 0.0263, Validation Loss: 0.0463
Best model saved!
Epoch 30/100
Epoch [30/100], Training Loss: 0.0271, Validation Loss: 0.0456
Best model saved!
Epoch 31/100
Epoch [31/100], Training Loss: 0.0270, Validation Loss: 0.0450
Best model saved!
Epoch 32/100
Epoch [32/100], Training Loss: 0.0257, Validation Loss: 0.0445
Best model saved!
Epoch 33/100
Epoch [33/100], Training Loss: 0.0258, Validation Loss: 0.0434
Best model saved!
Epoch 34/100
Epoch [34/100], Training Loss: 0.0253, Validation Loss: 0.0423
Best model saved!
Epoch 35/100
Epoch [35/100], Training Loss: 0.0249, Validation Loss: 0.0419
Best model saved!
Epoch 36/100
Epoch [36/100], Training Loss: 0.0249, Validation Loss: 0.0415
Best model saved!
Epoch 37/100
Epoch [37/100], Training Loss: 0.0244, Validation Loss: 0.0407
Best model saved!
Epoch 38/100
Epoch [38/100], Training Loss: 0.0242, Validation Loss: 0.0393
Best model saved!
Epoch 39/100
Epoch [39/100], Training Loss: 0.0238, Validation Loss: 0.0384
Best model saved!
Epoch 40/100
Epoch [40/100], Training Loss: 0.0236, Validation Loss: 0.0386
Epoch 41/100
Epoch [41/100], Training Loss: 0.0233, Validation Loss: 0.0387
Epoch 42/100
Epoch [42/100], Training Loss: 0.0231, Validation Loss: 0.0381
Best model saved!
Epoch 43/100
Epoch [43/100], Training Loss: 0.0227, Validation Loss: 0.0371
Best model saved!
Epoch 44/100
Epoch [44/100], Training Loss: 0.0225, Validation Loss: 0.0362
Best model saved!
Epoch 45/100
Epoch [45/100], Training Loss: 0.0222, Validation Loss: 0.0361
Best model saved!
Epoch 46/100
Epoch [46/100], Training Loss: 0.0220, Validation Loss: 0.0355
Best model saved!
Epoch 47/100
Epoch [47/100], Training Loss: 0.0220, Validation Loss: 0.0360
Epoch 48/100
Epoch [48/100], Training Loss: 0.0221, Validation Loss: 0.0348
Best model saved!
Epoch 49/100
Epoch [49/100], Training Loss: 0.0223, Validation Loss: 0.0351
Epoch 50/100
Epoch [50/100], Training Loss: 0.0214, Validation Loss: 0.0346
Best model saved!
Epoch 51/100
Epoch [51/100], Training Loss: 0.0212, Validation Loss: 0.0336
Best model saved!
Epoch 52/100
Epoch [52/100], Training Loss: 0.0210, Validation Loss: 0.0332
Best model saved!
Epoch 53/100
Epoch [53/100], Training Loss: 0.0207, Validation Loss: 0.0327
Best model saved!
Epoch 54/100
Epoch [54/100], Training Loss: 0.0205, Validation Loss: 0.0329
Epoch 55/100
Epoch [55/100], Training Loss: 0.0202, Validation Loss: 0.0327
Epoch 56/100
Epoch [56/100], Training Loss: 0.0200, Validation Loss: 0.0316
Best model saved!
Epoch 57/100
Epoch [57/100], Training Loss: 0.0198, Validation Loss: 0.0318
Epoch 58/100
Epoch [58/100], Training Loss: 0.0195, Validation Loss: 0.0317
Epoch 59/100
Epoch [59/100], Training Loss: 0.0193, Validation Loss: 0.0311
Best model saved!
Epoch 60/100
Epoch [60/100], Training Loss: 0.0191, Validation Loss: 0.0314
Epoch 61/100
Epoch [61/100], Training Loss: 0.0189, Validation Loss: 0.0308
Best model saved!
Epoch 62/100
Epoch [62/100], Training Loss: 0.0186, Validation Loss: 0.0306
Best model saved!
Epoch 63/100
Epoch [63/100], Training Loss: 0.0184, Validation Loss: 0.0304
Best model saved!
Epoch 64/100
Epoch [64/100], Training Loss: 0.0182, Validation Loss: 0.0302
Best model saved!
Epoch 65/100
Epoch [65/100], Training Loss: 0.0180, Validation Loss: 0.0304
Epoch 66/100
Epoch [66/100], Training Loss: 0.0178, Validation Loss: 0.0300
Best model saved!
Epoch 67/100
Epoch [67/100], Training Loss: 0.0178, Validation Loss: 0.0311
Epoch 68/100
Epoch [68/100], Training Loss: 0.0184, Validation Loss: 0.0317
Epoch 69/100
Epoch [69/100], Training Loss: 0.0197, Validation Loss: 0.0312
Epoch 70/100
Epoch [70/100], Training Loss: 0.0185, Validation Loss: 0.0321
Epoch 71/100
Epoch [71/100], Training Loss: 0.0181, Validation Loss: 0.0298
Best model saved!
Epoch 72/100
Epoch [72/100], Training Loss: 0.0178, Validation Loss: 0.0290
Best model saved!
Epoch 73/100
Epoch [73/100], Training Loss: 0.0174, Validation Loss: 0.0295
Epoch 74/100
Epoch [74/100], Training Loss: 0.0173, Validation Loss: 0.0299
Epoch 75/100
Epoch [75/100], Training Loss: 0.0169, Validation Loss: 0.0290
Best model saved!
Epoch 76/100
Epoch [76/100], Training Loss: 0.0169, Validation Loss: 0.0284
Best model saved!
Epoch 77/100
Epoch [77/100], Training Loss: 0.0163, Validation Loss: 0.0291
Epoch 78/100
Epoch [78/100], Training Loss: 0.0164, Validation Loss: 0.0291
Epoch 79/100
Epoch [79/100], Training Loss: 0.0160, Validation Loss: 0.0286
Epoch 80/100
Epoch [80/100], Training Loss: 0.0159, Validation Loss: 0.0284
Epoch 81/100
Epoch [81/100], Training Loss: 0.0156, Validation Loss: 0.0288
Epoch 82/100
Epoch [82/100], Training Loss: 0.0155, Validation Loss: 0.0287
Epoch 83/100
Epoch [83/100], Training Loss: 0.0152, Validation Loss: 0.0284
Epoch 84/100
Epoch [84/100], Training Loss: 0.0150, Validation Loss: 0.0280
Best model saved!
Epoch 85/100
Epoch [85/100], Training Loss: 0.0148, Validation Loss: 0.0285
Epoch 86/100
Epoch [86/100], Training Loss: 0.0146, Validation Loss: 0.0285
Epoch 87/100
Epoch [87/100], Training Loss: 0.0145, Validation Loss: 0.0286
Epoch 88/100
Epoch [88/100], Training Loss: 0.0146, Validation Loss: 0.0283
Epoch 89/100
Epoch [89/100], Training Loss: 0.0146, Validation Loss: 0.0287
Epoch 90/100
Epoch [90/100], Training Loss: 0.0141, Validation Loss: 0.0281
Epoch 91/100
Epoch [91/100], Training Loss: 0.0142, Validation Loss: 0.0286
Epoch 92/100
Epoch [92/100], Training Loss: 0.0138, Validation Loss: 0.0279
Best model saved!
Epoch 93/100
Epoch [93/100], Training Loss: 0.0138, Validation Loss: 0.0276
Best model saved!
Epoch 94/100
Epoch [94/100], Training Loss: 0.0135, Validation Loss: 0.0275
Best model saved!
Epoch 95/100
Epoch [95/100], Training Loss: 0.0132, Validation Loss: 0.0276
Epoch 96/100
Epoch [96/100], Training Loss: 0.0130, Validation Loss: 0.0274
Best model saved!
Epoch 97/100
Epoch [97/100], Training Loss: 0.0129, Validation Loss: 0.0271
Best model saved!
Epoch 98/100
Epoch [98/100], Training Loss: 0.0128, Validation Loss: 0.0278
Epoch 99/100
Epoch [99/100], Training Loss: 0.0126, Validation Loss: 0.0270
Best model saved!
Epoch 100/100
Epoch [100/100], Training Loss: 0.0125, Validation Loss: 0.0279
Test Loss: 0.0172
done with hyperparameter tuning training
Model type: UNet6, Epochs: 100, Learning rate: 0.0001, Batch size: 64
Hyperparameter tuning prediction finished
UNet6 with 0.4 0.3 100 0.0001 64 small done at Fri Nov  1 19:34:44 CET 2024
UNet6 with 0.4 0.3 150 0.0001 64 small start at Fri Nov  1 19:34:44 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 150 learning_rate: 0.0001 batch_size: 64 model_type: UNet6
Epoch 1/150
Epoch [1/150], Training Loss: 0.0500, Validation Loss: 0.0523
Best model saved!
Epoch 2/150
Epoch [2/150], Training Loss: 0.0471, Validation Loss: 0.0523
Best model saved!
Epoch 3/150
Epoch [3/150], Training Loss: 0.0454, Validation Loss: 0.0522
Best model saved!
Epoch 4/150
Epoch [4/150], Training Loss: 0.0440, Validation Loss: 0.0522
Best model saved!
Epoch 5/150
Epoch [5/150], Training Loss: 0.0429, Validation Loss: 0.0521
Best model saved!
Epoch 6/150
Epoch [6/150], Training Loss: 0.0419, Validation Loss: 0.0521
Best model saved!
Epoch 7/150
Epoch [7/150], Training Loss: 0.0410, Validation Loss: 0.0520
Best model saved!
Epoch 8/150
Epoch [8/150], Training Loss: 0.0399, Validation Loss: 0.0519
Best model saved!
Epoch 9/150
Epoch [9/150], Training Loss: 0.0385, Validation Loss: 0.0518
Best model saved!
Epoch 10/150
Epoch [10/150], Training Loss: 0.0373, Validation Loss: 0.0517
Best model saved!
Epoch 11/150
Epoch [11/150], Training Loss: 0.0384, Validation Loss: 0.0516
Best model saved!
Epoch 12/150
Epoch [12/150], Training Loss: 0.0358, Validation Loss: 0.0515
Best model saved!
Epoch 13/150
Epoch [13/150], Training Loss: 0.0352, Validation Loss: 0.0514
Best model saved!
Epoch 14/150
Epoch [14/150], Training Loss: 0.0339, Validation Loss: 0.0512
Best model saved!
Epoch 15/150
Epoch [15/150], Training Loss: 0.0334, Validation Loss: 0.0511
Best model saved!
Epoch 16/150
Epoch [16/150], Training Loss: 0.0325, Validation Loss: 0.0509
Best model saved!
Epoch 17/150
Epoch [17/150], Training Loss: 0.0317, Validation Loss: 0.0507
Best model saved!
Epoch 18/150
Epoch [18/150], Training Loss: 0.0313, Validation Loss: 0.0505
Best model saved!
Epoch 19/150
Epoch [19/150], Training Loss: 0.0305, Validation Loss: 0.0502
Best model saved!
Epoch 20/150
Epoch [20/150], Training Loss: 0.0300, Validation Loss: 0.0499
Best model saved!
Epoch 21/150
Epoch [21/150], Training Loss: 0.0296, Validation Loss: 0.0496
Best model saved!
Epoch 22/150
Epoch [22/150], Training Loss: 0.0290, Validation Loss: 0.0494
Best model saved!
Epoch 23/150
Epoch [23/150], Training Loss: 0.0285, Validation Loss: 0.0489
Best model saved!
Epoch 24/150
Epoch [24/150], Training Loss: 0.0279, Validation Loss: 0.0486
Best model saved!
Epoch 25/150
Epoch [25/150], Training Loss: 0.0275, Validation Loss: 0.0482
Best model saved!
Epoch 26/150
Epoch [26/150], Training Loss: 0.0271, Validation Loss: 0.0476
Best model saved!
Epoch 27/150
Epoch [27/150], Training Loss: 0.0267, Validation Loss: 0.0473
Best model saved!
Epoch 28/150
Epoch [28/150], Training Loss: 0.0264, Validation Loss: 0.0466
Best model saved!
Epoch 29/150
Epoch [29/150], Training Loss: 0.0263, Validation Loss: 0.0463
Best model saved!
Epoch 30/150
Epoch [30/150], Training Loss: 0.0271, Validation Loss: 0.0456
Best model saved!
Epoch 31/150
Epoch [31/150], Training Loss: 0.0270, Validation Loss: 0.0450
Best model saved!
Epoch 32/150
Epoch [32/150], Training Loss: 0.0257, Validation Loss: 0.0445
Best model saved!
Epoch 33/150
Epoch [33/150], Training Loss: 0.0258, Validation Loss: 0.0434
Best model saved!
Epoch 34/150
Epoch [34/150], Training Loss: 0.0253, Validation Loss: 0.0423
Best model saved!
Epoch 35/150
Epoch [35/150], Training Loss: 0.0249, Validation Loss: 0.0419
Best model saved!
Epoch 36/150
Epoch [36/150], Training Loss: 0.0249, Validation Loss: 0.0415
Best model saved!
Epoch 37/150
Epoch [37/150], Training Loss: 0.0244, Validation Loss: 0.0407
Best model saved!
Epoch 38/150
Epoch [38/150], Training Loss: 0.0242, Validation Loss: 0.0393
Best model saved!
Epoch 39/150
Epoch [39/150], Training Loss: 0.0238, Validation Loss: 0.0384
Best model saved!
Epoch 40/150
Epoch [40/150], Training Loss: 0.0236, Validation Loss: 0.0386
Epoch 41/150
Epoch [41/150], Training Loss: 0.0233, Validation Loss: 0.0387
Epoch 42/150
Epoch [42/150], Training Loss: 0.0231, Validation Loss: 0.0381
Best model saved!
Epoch 43/150
Epoch [43/150], Training Loss: 0.0227, Validation Loss: 0.0371
Best model saved!
Epoch 44/150
Epoch [44/150], Training Loss: 0.0225, Validation Loss: 0.0362
Best model saved!
Epoch 45/150
Epoch [45/150], Training Loss: 0.0222, Validation Loss: 0.0361
Best model saved!
Epoch 46/150
Epoch [46/150], Training Loss: 0.0220, Validation Loss: 0.0355
Best model saved!
Epoch 47/150
Epoch [47/150], Training Loss: 0.0220, Validation Loss: 0.0360
Epoch 48/150
Epoch [48/150], Training Loss: 0.0221, Validation Loss: 0.0348
Best model saved!
Epoch 49/150
Epoch [49/150], Training Loss: 0.0223, Validation Loss: 0.0351
Epoch 50/150
Epoch [50/150], Training Loss: 0.0214, Validation Loss: 0.0346
Best model saved!
Epoch 51/150
Epoch [51/150], Training Loss: 0.0212, Validation Loss: 0.0336
Best model saved!
Epoch 52/150
Epoch [52/150], Training Loss: 0.0210, Validation Loss: 0.0332
Best model saved!
Epoch 53/150
Epoch [53/150], Training Loss: 0.0207, Validation Loss: 0.0327
Best model saved!
Epoch 54/150
Epoch [54/150], Training Loss: 0.0205, Validation Loss: 0.0329
Epoch 55/150
Epoch [55/150], Training Loss: 0.0202, Validation Loss: 0.0327
Epoch 56/150
Epoch [56/150], Training Loss: 0.0200, Validation Loss: 0.0316
Best model saved!
Epoch 57/150
Epoch [57/150], Training Loss: 0.0198, Validation Loss: 0.0318
Epoch 58/150
Epoch [58/150], Training Loss: 0.0195, Validation Loss: 0.0317
Epoch 59/150
Epoch [59/150], Training Loss: 0.0193, Validation Loss: 0.0311
Best model saved!
Epoch 60/150
Epoch [60/150], Training Loss: 0.0191, Validation Loss: 0.0314
Epoch 61/150
Epoch [61/150], Training Loss: 0.0189, Validation Loss: 0.0308
Best model saved!
Epoch 62/150
Epoch [62/150], Training Loss: 0.0186, Validation Loss: 0.0306
Best model saved!
Epoch 63/150
Epoch [63/150], Training Loss: 0.0184, Validation Loss: 0.0304
Best model saved!
Epoch 64/150
Epoch [64/150], Training Loss: 0.0182, Validation Loss: 0.0302
Best model saved!
Epoch 65/150
Epoch [65/150], Training Loss: 0.0180, Validation Loss: 0.0304
Epoch 66/150
Epoch [66/150], Training Loss: 0.0178, Validation Loss: 0.0300
Best model saved!
Epoch 67/150
Epoch [67/150], Training Loss: 0.0178, Validation Loss: 0.0311
Epoch 68/150
Epoch [68/150], Training Loss: 0.0184, Validation Loss: 0.0317
Epoch 69/150
Epoch [69/150], Training Loss: 0.0197, Validation Loss: 0.0312
Epoch 70/150
Epoch [70/150], Training Loss: 0.0185, Validation Loss: 0.0321
Epoch 71/150
Epoch [71/150], Training Loss: 0.0181, Validation Loss: 0.0298
Best model saved!
Epoch 72/150
Epoch [72/150], Training Loss: 0.0178, Validation Loss: 0.0290
Best model saved!
Epoch 73/150
Epoch [73/150], Training Loss: 0.0174, Validation Loss: 0.0295
Epoch 74/150
Epoch [74/150], Training Loss: 0.0173, Validation Loss: 0.0299
Epoch 75/150
Epoch [75/150], Training Loss: 0.0169, Validation Loss: 0.0290
Best model saved!
Epoch 76/150
Epoch [76/150], Training Loss: 0.0169, Validation Loss: 0.0284
Best model saved!
Epoch 77/150
Epoch [77/150], Training Loss: 0.0163, Validation Loss: 0.0291
Epoch 78/150
Epoch [78/150], Training Loss: 0.0164, Validation Loss: 0.0291
Epoch 79/150
Epoch [79/150], Training Loss: 0.0160, Validation Loss: 0.0286
Epoch 80/150
Epoch [80/150], Training Loss: 0.0159, Validation Loss: 0.0284
Epoch 81/150
Epoch [81/150], Training Loss: 0.0156, Validation Loss: 0.0288
Epoch 82/150
Epoch [82/150], Training Loss: 0.0155, Validation Loss: 0.0287
Epoch 83/150
Epoch [83/150], Training Loss: 0.0152, Validation Loss: 0.0284
Epoch 84/150
Epoch [84/150], Training Loss: 0.0150, Validation Loss: 0.0280
Best model saved!
Epoch 85/150
Epoch [85/150], Training Loss: 0.0148, Validation Loss: 0.0285
Epoch 86/150
Epoch [86/150], Training Loss: 0.0146, Validation Loss: 0.0285
Epoch 87/150
Epoch [87/150], Training Loss: 0.0145, Validation Loss: 0.0286
Epoch 88/150
Epoch [88/150], Training Loss: 0.0146, Validation Loss: 0.0283
Epoch 89/150
Epoch [89/150], Training Loss: 0.0146, Validation Loss: 0.0287
Epoch 90/150
Epoch [90/150], Training Loss: 0.0141, Validation Loss: 0.0281
Epoch 91/150
Epoch [91/150], Training Loss: 0.0142, Validation Loss: 0.0286
Epoch 92/150
Epoch [92/150], Training Loss: 0.0138, Validation Loss: 0.0279
Best model saved!
Epoch 93/150
Epoch [93/150], Training Loss: 0.0138, Validation Loss: 0.0276
Best model saved!
Epoch 94/150
Epoch [94/150], Training Loss: 0.0135, Validation Loss: 0.0275
Best model saved!
Epoch 95/150
Epoch [95/150], Training Loss: 0.0132, Validation Loss: 0.0276
Epoch 96/150
Epoch [96/150], Training Loss: 0.0130, Validation Loss: 0.0274
Best model saved!
Epoch 97/150
Epoch [97/150], Training Loss: 0.0129, Validation Loss: 0.0271
Best model saved!
Epoch 98/150
Epoch [98/150], Training Loss: 0.0128, Validation Loss: 0.0278
Epoch 99/150
Epoch [99/150], Training Loss: 0.0126, Validation Loss: 0.0270
Best model saved!
Epoch 100/150
Epoch [100/150], Training Loss: 0.0125, Validation Loss: 0.0279
Epoch 101/150
Epoch [101/150], Training Loss: 0.0125, Validation Loss: 0.0274
Epoch 102/150
Epoch [102/150], Training Loss: 0.0127, Validation Loss: 0.0289
Epoch 103/150
Epoch [103/150], Training Loss: 0.0130, Validation Loss: 0.0279
Epoch 104/150
Epoch [104/150], Training Loss: 0.0126, Validation Loss: 0.0269
Best model saved!
Epoch 105/150
Epoch [105/150], Training Loss: 0.0121, Validation Loss: 0.0275
Epoch 106/150
Epoch [106/150], Training Loss: 0.0121, Validation Loss: 0.0274
Epoch 107/150
Epoch [107/150], Training Loss: 0.0119, Validation Loss: 0.0268
Best model saved!
Epoch 108/150
Epoch [108/150], Training Loss: 0.0115, Validation Loss: 0.0270
Epoch 109/150
Epoch [109/150], Training Loss: 0.0115, Validation Loss: 0.0269
Epoch 110/150
Epoch [110/150], Training Loss: 0.0112, Validation Loss: 0.0271
Epoch 111/150
Epoch [111/150], Training Loss: 0.0110, Validation Loss: 0.0269
Epoch 112/150
Epoch [112/150], Training Loss: 0.0108, Validation Loss: 0.0268
Best model saved!
Epoch 113/150
Epoch [113/150], Training Loss: 0.0107, Validation Loss: 0.0269
Epoch 114/150
Epoch [114/150], Training Loss: 0.0105, Validation Loss: 0.0267
Best model saved!
Epoch 115/150
Epoch [115/150], Training Loss: 0.0102, Validation Loss: 0.0267
Epoch 116/150
Epoch [116/150], Training Loss: 0.0102, Validation Loss: 0.0269
Epoch 117/150
Epoch [117/150], Training Loss: 0.0101, Validation Loss: 0.0265
Best model saved!
Epoch 118/150
Epoch [118/150], Training Loss: 0.0099, Validation Loss: 0.0274
Epoch 119/150
Epoch [119/150], Training Loss: 0.0102, Validation Loss: 0.0269
Epoch 120/150
Epoch [120/150], Training Loss: 0.0104, Validation Loss: 0.0282
Epoch 121/150
Epoch [121/150], Training Loss: 0.0105, Validation Loss: 0.0270
Epoch 122/150
Epoch [122/150], Training Loss: 0.0101, Validation Loss: 0.0265
Best model saved!
Epoch 123/150
Epoch [123/150], Training Loss: 0.0102, Validation Loss: 0.0273
Epoch 124/150
Epoch [124/150], Training Loss: 0.0098, Validation Loss: 0.0272
Epoch 125/150
Epoch [125/150], Training Loss: 0.0095, Validation Loss: 0.0260
Best model saved!
Epoch 126/150
Epoch [126/150], Training Loss: 0.0094, Validation Loss: 0.0263
Epoch 127/150
Epoch [127/150], Training Loss: 0.0091, Validation Loss: 0.0270
Epoch 128/150
Epoch [128/150], Training Loss: 0.0090, Validation Loss: 0.0261
Epoch 129/150
Epoch [129/150], Training Loss: 0.0089, Validation Loss: 0.0266
Epoch 130/150
Epoch [130/150], Training Loss: 0.0087, Validation Loss: 0.0264
Epoch 131/150
Epoch [131/150], Training Loss: 0.0086, Validation Loss: 0.0265
Epoch 132/150
Epoch [132/150], Training Loss: 0.0085, Validation Loss: 0.0263
Epoch 133/150
Epoch [133/150], Training Loss: 0.0084, Validation Loss: 0.0268
Epoch 134/150
Epoch [134/150], Training Loss: 0.0084, Validation Loss: 0.0262
Epoch 135/150
Epoch [135/150], Training Loss: 0.0084, Validation Loss: 0.0268
Epoch 136/150
Epoch [136/150], Training Loss: 0.0084, Validation Loss: 0.0262
Epoch 137/150
Epoch [137/150], Training Loss: 0.0081, Validation Loss: 0.0263
Epoch 138/150
Epoch [138/150], Training Loss: 0.0078, Validation Loss: 0.0263
Epoch 139/150
Epoch [139/150], Training Loss: 0.0081, Validation Loss: 0.0268
Epoch 140/150
Epoch [140/150], Training Loss: 0.0085, Validation Loss: 0.0270
Epoch 141/150
Epoch [141/150], Training Loss: 0.0085, Validation Loss: 0.0263
Epoch 142/150
Epoch [142/150], Training Loss: 0.0080, Validation Loss: 0.0268
Epoch 143/150
Epoch [143/150], Training Loss: 0.0078, Validation Loss: 0.0258
Best model saved!
Epoch 144/150
Epoch [144/150], Training Loss: 0.0079, Validation Loss: 0.0265
Epoch 145/150
Epoch [145/150], Training Loss: 0.0078, Validation Loss: 0.0269
Epoch 146/150
Epoch [146/150], Training Loss: 0.0076, Validation Loss: 0.0260
Epoch 147/150
Epoch [147/150], Training Loss: 0.0075, Validation Loss: 0.0266
Epoch 148/150
Epoch [148/150], Training Loss: 0.0074, Validation Loss: 0.0265
Epoch 149/150
Epoch [149/150], Training Loss: 0.0074, Validation Loss: 0.0266
Epoch 150/150
Epoch [150/150], Training Loss: 0.0073, Validation Loss: 0.0262
Test Loss: 0.0166
done with hyperparameter tuning training
Model type: UNet6, Epochs: 150, Learning rate: 0.0001, Batch size: 64
Hyperparameter tuning prediction finished
UNet6 with 0.4 0.3 150 0.0001 64 small done at Fri Nov  1 23:21:40 CET 2024
UNet6 with 0.4 0.3 200 0.0001 64 small start at Fri Nov  1 23:21:40 CET 2024
current path /eejit/home/hausw001/HybGGM/hybGGM_test
data testing folder already prepared
data testing folder already prepared
Hyperparameter tuning definition and start
testSize: 0.4 trainSize: 0.3 epochs: 200 learning_rate: 0.0001 batch_size: 64 model_type: UNet6
Epoch 1/200
Epoch [1/200], Training Loss: 0.0500, Validation Loss: 0.0523
Best model saved!
Epoch 2/200
Epoch [2/200], Training Loss: 0.0471, Validation Loss: 0.0523
Best model saved!
Epoch 3/200
Epoch [3/200], Training Loss: 0.0454, Validation Loss: 0.0522
Best model saved!
Epoch 4/200
Epoch [4/200], Training Loss: 0.0440, Validation Loss: 0.0522
Best model saved!
Epoch 5/200
Epoch [5/200], Training Loss: 0.0429, Validation Loss: 0.0521
Best model saved!
Epoch 6/200
Epoch [6/200], Training Loss: 0.0419, Validation Loss: 0.0521
Best model saved!
Epoch 7/200
Epoch [7/200], Training Loss: 0.0410, Validation Loss: 0.0520
Best model saved!
Epoch 8/200
Epoch [8/200], Training Loss: 0.0399, Validation Loss: 0.0519
Best model saved!
Epoch 9/200
Epoch [9/200], Training Loss: 0.0385, Validation Loss: 0.0518
Best model saved!
Epoch 10/200
Epoch [10/200], Training Loss: 0.0373, Validation Loss: 0.0517
Best model saved!
Epoch 11/200
Epoch [11/200], Training Loss: 0.0384, Validation Loss: 0.0516
Best model saved!
Epoch 12/200
Epoch [12/200], Training Loss: 0.0358, Validation Loss: 0.0515
Best model saved!
Epoch 13/200
Epoch [13/200], Training Loss: 0.0352, Validation Loss: 0.0514
Best model saved!
Epoch 14/200
Epoch [14/200], Training Loss: 0.0339, Validation Loss: 0.0512
Best model saved!
Epoch 15/200
Epoch [15/200], Training Loss: 0.0334, Validation Loss: 0.0511
Best model saved!
Epoch 16/200
Epoch [16/200], Training Loss: 0.0325, Validation Loss: 0.0509
Best model saved!
Epoch 17/200
Epoch [17/200], Training Loss: 0.0317, Validation Loss: 0.0507
Best model saved!
Epoch 18/200
Epoch [18/200], Training Loss: 0.0313, Validation Loss: 0.0505
Best model saved!
Epoch 19/200
Epoch [19/200], Training Loss: 0.0305, Validation Loss: 0.0502
Best model saved!
Epoch 20/200
Epoch [20/200], Training Loss: 0.0300, Validation Loss: 0.0499
Best model saved!
Epoch 21/200
Epoch [21/200], Training Loss: 0.0296, Validation Loss: 0.0496
Best model saved!
Epoch 22/200
Epoch [22/200], Training Loss: 0.0290, Validation Loss: 0.0494
Best model saved!
Epoch 23/200
Epoch [23/200], Training Loss: 0.0285, Validation Loss: 0.0489
Best model saved!
Epoch 24/200
Epoch [24/200], Training Loss: 0.0279, Validation Loss: 0.0486
Best model saved!
Epoch 25/200
Epoch [25/200], Training Loss: 0.0275, Validation Loss: 0.0482
Best model saved!
Epoch 26/200
Epoch [26/200], Training Loss: 0.0271, Validation Loss: 0.0476
Best model saved!
Epoch 27/200
Epoch [27/200], Training Loss: 0.0267, Validation Loss: 0.0473
Best model saved!
Epoch 28/200
Epoch [28/200], Training Loss: 0.0264, Validation Loss: 0.0466
Best model saved!
Epoch 29/200
Epoch [29/200], Training Loss: 0.0263, Validation Loss: 0.0463
Best model saved!
Epoch 30/200
Epoch [30/200], Training Loss: 0.0271, Validation Loss: 0.0456
Best model saved!
Epoch 31/200
Epoch [31/200], Training Loss: 0.0270, Validation Loss: 0.0450
Best model saved!
Epoch 32/200
Epoch [32/200], Training Loss: 0.0257, Validation Loss: 0.0445
Best model saved!
Epoch 33/200
Epoch [33/200], Training Loss: 0.0258, Validation Loss: 0.0434
Best model saved!
Epoch 34/200
Epoch [34/200], Training Loss: 0.0253, Validation Loss: 0.0423
Best model saved!
Epoch 35/200
Epoch [35/200], Training Loss: 0.0249, Validation Loss: 0.0419
Best model saved!
Epoch 36/200
Epoch [36/200], Training Loss: 0.0249, Validation Loss: 0.0415
Best model saved!
Epoch 37/200
Epoch [37/200], Training Loss: 0.0244, Validation Loss: 0.0407
Best model saved!
Epoch 38/200
Epoch [38/200], Training Loss: 0.0242, Validation Loss: 0.0393
Best model saved!
Epoch 39/200
Epoch [39/200], Training Loss: 0.0238, Validation Loss: 0.0384
Best model saved!
Epoch 40/200
Epoch [40/200], Training Loss: 0.0236, Validation Loss: 0.0386
Epoch 41/200
Epoch [41/200], Training Loss: 0.0233, Validation Loss: 0.0387
Epoch 42/200
Epoch [42/200], Training Loss: 0.0231, Validation Loss: 0.0381
Best model saved!
Epoch 43/200
Epoch [43/200], Training Loss: 0.0227, Validation Loss: 0.0371
Best model saved!
Epoch 44/200
Epoch [44/200], Training Loss: 0.0225, Validation Loss: 0.0362
Best model saved!
Epoch 45/200
Epoch [45/200], Training Loss: 0.0222, Validation Loss: 0.0361
Best model saved!
Epoch 46/200
Epoch [46/200], Training Loss: 0.0220, Validation Loss: 0.0355
Best model saved!
Epoch 47/200
Epoch [47/200], Training Loss: 0.0220, Validation Loss: 0.0360
Epoch 48/200
Epoch [48/200], Training Loss: 0.0221, Validation Loss: 0.0348
Best model saved!
Epoch 49/200
Epoch [49/200], Training Loss: 0.0223, Validation Loss: 0.0351
Epoch 50/200
Epoch [50/200], Training Loss: 0.0214, Validation Loss: 0.0346
Best model saved!
Epoch 51/200
Epoch [51/200], Training Loss: 0.0212, Validation Loss: 0.0336
Best model saved!
Epoch 52/200
Epoch [52/200], Training Loss: 0.0210, Validation Loss: 0.0332
Best model saved!
Epoch 53/200
Epoch [53/200], Training Loss: 0.0207, Validation Loss: 0.0327
Best model saved!
Epoch 54/200
Epoch [54/200], Training Loss: 0.0205, Validation Loss: 0.0329
Epoch 55/200
Epoch [55/200], Training Loss: 0.0202, Validation Loss: 0.0327
Epoch 56/200
Epoch [56/200], Training Loss: 0.0200, Validation Loss: 0.0316
Best model saved!
Epoch 57/200
Epoch [57/200], Training Loss: 0.0198, Validation Loss: 0.0318
Epoch 58/200
Epoch [58/200], Training Loss: 0.0195, Validation Loss: 0.0317
Epoch 59/200
Epoch [59/200], Training Loss: 0.0193, Validation Loss: 0.0311
Best model saved!
Epoch 60/200
Epoch [60/200], Training Loss: 0.0191, Validation Loss: 0.0314
Epoch 61/200
Epoch [61/200], Training Loss: 0.0189, Validation Loss: 0.0308
Best model saved!
Epoch 62/200
Epoch [62/200], Training Loss: 0.0186, Validation Loss: 0.0306
Best model saved!
Epoch 63/200
Epoch [63/200], Training Loss: 0.0184, Validation Loss: 0.0304
Best model saved!
Epoch 64/200
Epoch [64/200], Training Loss: 0.0182, Validation Loss: 0.0302
Best model saved!
Epoch 65/200
Epoch [65/200], Training Loss: 0.0180, Validation Loss: 0.0304
Epoch 66/200
Epoch [66/200], Training Loss: 0.0178, Validation Loss: 0.0300
Best model saved!
Epoch 67/200
Epoch [67/200], Training Loss: 0.0178, Validation Loss: 0.0311
Epoch 68/200
Epoch [68/200], Training Loss: 0.0184, Validation Loss: 0.0317
Epoch 69/200
Epoch [69/200], Training Loss: 0.0197, Validation Loss: 0.0312
Epoch 70/200
Epoch [70/200], Training Loss: 0.0185, Validation Loss: 0.0321
Epoch 71/200
Epoch [71/200], Training Loss: 0.0181, Validation Loss: 0.0298
Best model saved!
Epoch 72/200
Epoch [72/200], Training Loss: 0.0178, Validation Loss: 0.0290
Best model saved!
Epoch 73/200
Epoch [73/200], Training Loss: 0.0174, Validation Loss: 0.0295
Epoch 74/200
Epoch [74/200], Training Loss: 0.0173, Validation Loss: 0.0299
Epoch 75/200
Epoch [75/200], Training Loss: 0.0169, Validation Loss: 0.0290
Best model saved!
Epoch 76/200
Epoch [76/200], Training Loss: 0.0169, Validation Loss: 0.0284
Best model saved!
Epoch 77/200
Epoch [77/200], Training Loss: 0.0163, Validation Loss: 0.0291
Epoch 78/200
Epoch [78/200], Training Loss: 0.0164, Validation Loss: 0.0291
Epoch 79/200
Epoch [79/200], Training Loss: 0.0160, Validation Loss: 0.0286
Epoch 80/200
Epoch [80/200], Training Loss: 0.0159, Validation Loss: 0.0284
Epoch 81/200
Epoch [81/200], Training Loss: 0.0156, Validation Loss: 0.0288
Epoch 82/200
Epoch [82/200], Training Loss: 0.0155, Validation Loss: 0.0287
Epoch 83/200
Epoch [83/200], Training Loss: 0.0152, Validation Loss: 0.0284
Epoch 84/200
Epoch [84/200], Training Loss: 0.0150, Validation Loss: 0.0280
Best model saved!
Epoch 85/200
Epoch [85/200], Training Loss: 0.0148, Validation Loss: 0.0285
Epoch 86/200
Epoch [86/200], Training Loss: 0.0146, Validation Loss: 0.0285
Epoch 87/200
Epoch [87/200], Training Loss: 0.0145, Validation Loss: 0.0286
Epoch 88/200
Epoch [88/200], Training Loss: 0.0146, Validation Loss: 0.0283
Epoch 89/200
Epoch [89/200], Training Loss: 0.0146, Validation Loss: 0.0287
Epoch 90/200
Epoch [90/200], Training Loss: 0.0141, Validation Loss: 0.0281
Epoch 91/200
Epoch [91/200], Training Loss: 0.0142, Validation Loss: 0.0286
Epoch 92/200
Epoch [92/200], Training Loss: 0.0138, Validation Loss: 0.0279
Best model saved!
Epoch 93/200
Epoch [93/200], Training Loss: 0.0138, Validation Loss: 0.0276
Best model saved!
Epoch 94/200
Epoch [94/200], Training Loss: 0.0135, Validation Loss: 0.0275
Best model saved!
Epoch 95/200
Epoch [95/200], Training Loss: 0.0132, Validation Loss: 0.0276
Epoch 96/200
Epoch [96/200], Training Loss: 0.0130, Validation Loss: 0.0274
Best model saved!
Epoch 97/200
Epoch [97/200], Training Loss: 0.0129, Validation Loss: 0.0271
Best model saved!
Epoch 98/200
Epoch [98/200], Training Loss: 0.0128, Validation Loss: 0.0278
Epoch 99/200
Epoch [99/200], Training Loss: 0.0126, Validation Loss: 0.0270
Best model saved!
Epoch 100/200
Epoch [100/200], Training Loss: 0.0125, Validation Loss: 0.0279
Epoch 101/200
Epoch [101/200], Training Loss: 0.0125, Validation Loss: 0.0274
Epoch 102/200
Epoch [102/200], Training Loss: 0.0127, Validation Loss: 0.0289
Epoch 103/200
Epoch [103/200], Training Loss: 0.0130, Validation Loss: 0.0279
Epoch 104/200
Epoch [104/200], Training Loss: 0.0126, Validation Loss: 0.0269
Best model saved!
Epoch 105/200
Epoch [105/200], Training Loss: 0.0121, Validation Loss: 0.0275
Epoch 106/200
Epoch [106/200], Training Loss: 0.0121, Validation Loss: 0.0274
Epoch 107/200
Epoch [107/200], Training Loss: 0.0119, Validation Loss: 0.0268
Best model saved!
Epoch 108/200
Epoch [108/200], Training Loss: 0.0115, Validation Loss: 0.0270
Epoch 109/200
Epoch [109/200], Training Loss: 0.0115, Validation Loss: 0.0269
Epoch 110/200
Epoch [110/200], Training Loss: 0.0112, Validation Loss: 0.0271
Epoch 111/200
Epoch [111/200], Training Loss: 0.0110, Validation Loss: 0.0269
Epoch 112/200
Epoch [112/200], Training Loss: 0.0108, Validation Loss: 0.0268
Best model saved!
Epoch 113/200
Epoch [113/200], Training Loss: 0.0107, Validation Loss: 0.0269
Epoch 114/200
Epoch [114/200], Training Loss: 0.0105, Validation Loss: 0.0267
Best model saved!
Epoch 115/200
Epoch [115/200], Training Loss: 0.0102, Validation Loss: 0.0267
Epoch 116/200
Epoch [116/200], Training Loss: 0.0102, Validation Loss: 0.0269
Epoch 117/200
Epoch [117/200], Training Loss: 0.0101, Validation Loss: 0.0265
Best model saved!
Epoch 118/200
Epoch [118/200], Training Loss: 0.0099, Validation Loss: 0.0274
Epoch 119/200
Epoch [119/200], Training Loss: 0.0102, Validation Loss: 0.0269
Epoch 120/200
Epoch [120/200], Training Loss: 0.0104, Validation Loss: 0.0282
Epoch 121/200
Epoch [121/200], Training Loss: 0.0105, Validation Loss: 0.0270
Epoch 122/200
Epoch [122/200], Training Loss: 0.0101, Validation Loss: 0.0265
Best model saved!
Epoch 123/200
Epoch [123/200], Training Loss: 0.0102, Validation Loss: 0.0273
Epoch 124/200
Epoch [124/200], Training Loss: 0.0098, Validation Loss: 0.0272
Epoch 125/200
Epoch [125/200], Training Loss: 0.0095, Validation Loss: 0.0260
Best model saved!
Epoch 126/200
Epoch [126/200], Training Loss: 0.0094, Validation Loss: 0.0263
Epoch 127/200
Epoch [127/200], Training Loss: 0.0091, Validation Loss: 0.0270
Epoch 128/200
Epoch [128/200], Training Loss: 0.0090, Validation Loss: 0.0261
Epoch 129/200
Epoch [129/200], Training Loss: 0.0089, Validation Loss: 0.0266
Epoch 130/200
Epoch [130/200], Training Loss: 0.0087, Validation Loss: 0.0264
Epoch 131/200
Epoch [131/200], Training Loss: 0.0086, Validation Loss: 0.0265
Epoch 132/200
Epoch [132/200], Training Loss: 0.0085, Validation Loss: 0.0263
Epoch 133/200
Epoch [133/200], Training Loss: 0.0084, Validation Loss: 0.0268
Epoch 134/200
Epoch [134/200], Training Loss: 0.0084, Validation Loss: 0.0262
Epoch 135/200
Epoch [135/200], Training Loss: 0.0084, Validation Loss: 0.0268
Epoch 136/200
Epoch [136/200], Training Loss: 0.0084, Validation Loss: 0.0262
Epoch 137/200
Epoch [137/200], Training Loss: 0.0081, Validation Loss: 0.0263
Epoch 138/200
Epoch [138/200], Training Loss: 0.0078, Validation Loss: 0.0263
Epoch 139/200
Epoch [139/200], Training Loss: 0.0081, Validation Loss: 0.0268
Epoch 140/200
Epoch [140/200], Training Loss: 0.0085, Validation Loss: 0.0270
Epoch 141/200
Epoch [141/200], Training Loss: 0.0085, Validation Loss: 0.0263
Epoch 142/200
Epoch [142/200], Training Loss: 0.0080, Validation Loss: 0.0268
Epoch 143/200
Epoch [143/200], Training Loss: 0.0078, Validation Loss: 0.0258
Best model saved!
Epoch 144/200
Epoch [144/200], Training Loss: 0.0079, Validation Loss: 0.0265
Epoch 145/200
Epoch [145/200], Training Loss: 0.0078, Validation Loss: 0.0269
Epoch 146/200
Epoch [146/200], Training Loss: 0.0076, Validation Loss: 0.0260
Epoch 147/200
Epoch [147/200], Training Loss: 0.0075, Validation Loss: 0.0266
Epoch 148/200
Epoch [148/200], Training Loss: 0.0074, Validation Loss: 0.0265
Epoch 149/200
Epoch [149/200], Training Loss: 0.0074, Validation Loss: 0.0266
Epoch 150/200
Epoch [150/200], Training Loss: 0.0073, Validation Loss: 0.0262
Epoch 151/200
Epoch [151/200], Training Loss: 0.0070, Validation Loss: 0.0264
Epoch 152/200
Epoch [152/200], Training Loss: 0.0067, Validation Loss: 0.0262
Epoch 153/200
Epoch [153/200], Training Loss: 0.0066, Validation Loss: 0.0261
Epoch 154/200
Epoch [154/200], Training Loss: 0.0066, Validation Loss: 0.0264
Epoch 155/200
Epoch [155/200], Training Loss: 0.0066, Validation Loss: 0.0262
Epoch 156/200
Epoch [156/200], Training Loss: 0.0065, Validation Loss: 0.0265
Epoch 157/200
Epoch [157/200], Training Loss: 0.0066, Validation Loss: 0.0260
Epoch 158/200
Epoch [158/200], Training Loss: 0.0065, Validation Loss: 0.0264
Epoch 159/200
Epoch [159/200], Training Loss: 0.0063, Validation Loss: 0.0260
Epoch 160/200
Epoch [160/200], Training Loss: 0.0063, Validation Loss: 0.0262
Epoch 161/200
Epoch [161/200], Training Loss: 0.0063, Validation Loss: 0.0263
Epoch 162/200
Epoch [162/200], Training Loss: 0.0063, Validation Loss: 0.0261
Epoch 163/200
Epoch [163/200], Training Loss: 0.0063, Validation Loss: 0.0264
Epoch 164/200
Epoch [164/200], Training Loss: 0.0062, Validation Loss: 0.0258
Best model saved!
Epoch 165/200
Epoch [165/200], Training Loss: 0.0060, Validation Loss: 0.0266
Epoch 166/200
Epoch [166/200], Training Loss: 0.0059, Validation Loss: 0.0258
Epoch 167/200
Epoch [167/200], Training Loss: 0.0061, Validation Loss: 0.0274
Epoch 168/200
Epoch [168/200], Training Loss: 0.0065, Validation Loss: 0.0264
Epoch 169/200
Epoch [169/200], Training Loss: 0.0067, Validation Loss: 0.0267
Epoch 170/200
Epoch [170/200], Training Loss: 0.0063, Validation Loss: 0.0264
Epoch 171/200
Epoch [171/200], Training Loss: 0.0058, Validation Loss: 0.0260
Epoch 172/200
Epoch [172/200], Training Loss: 0.0056, Validation Loss: 0.0266
Epoch 173/200
Epoch [173/200], Training Loss: 0.0058, Validation Loss: 0.0263
Epoch 174/200
Epoch [174/200], Training Loss: 0.0059, Validation Loss: 0.0265
Epoch 175/200
Epoch [175/200], Training Loss: 0.0055, Validation Loss: 0.0263
Epoch 176/200
Epoch [176/200], Training Loss: 0.0055, Validation Loss: 0.0260
Epoch 177/200
Epoch [177/200], Training Loss: 0.0056, Validation Loss: 0.0268
Epoch 178/200
Epoch [178/200], Training Loss: 0.0056, Validation Loss: 0.0259
Epoch 179/200
Epoch [179/200], Training Loss: 0.0055, Validation Loss: 0.0266
Epoch 180/200
Epoch [180/200], Training Loss: 0.0054, Validation Loss: 0.0260
Epoch 181/200
Epoch [181/200], Training Loss: 0.0054, Validation Loss: 0.0263
Epoch 182/200
Epoch [182/200], Training Loss: 0.0054, Validation Loss: 0.0268
Epoch 183/200
Epoch [183/200], Training Loss: 0.0055, Validation Loss: 0.0261
Epoch 184/200
Epoch [184/200], Training Loss: 0.0052, Validation Loss: 0.0262
Epoch 185/200
Epoch [185/200], Training Loss: 0.0053, Validation Loss: 0.0266
Epoch 186/200
Epoch [186/200], Training Loss: 0.0053, Validation Loss: 0.0260
Epoch 187/200
Epoch [187/200], Training Loss: 0.0051, Validation Loss: 0.0266
Epoch 188/200
Epoch [188/200], Training Loss: 0.0049, Validation Loss: 0.0260
Epoch 189/200
Epoch [189/200], Training Loss: 0.0048, Validation Loss: 0.0265
Epoch 190/200
Epoch [190/200], Training Loss: 0.0049, Validation Loss: 0.0261
Epoch 191/200
Epoch [191/200], Training Loss: 0.0050, Validation Loss: 0.0266
Epoch 192/200
Epoch [192/200], Training Loss: 0.0050, Validation Loss: 0.0265
Epoch 193/200
Epoch [193/200], Training Loss: 0.0051, Validation Loss: 0.0265
Epoch 194/200
Epoch [194/200], Training Loss: 0.0052, Validation Loss: 0.0266
Epoch 195/200
Epoch [195/200], Training Loss: 0.0049, Validation Loss: 0.0257
Best model saved!
Epoch 196/200
Epoch [196/200], Training Loss: 0.0047, Validation Loss: 0.0269
Epoch 197/200
Epoch [197/200], Training Loss: 0.0048, Validation Loss: 0.0256
Best model saved!
Epoch 198/200
Epoch [198/200], Training Loss: 0.0049, Validation Loss: 0.0266
Epoch 199/200
Epoch [199/200], Training Loss: 0.0045, Validation Loss: 0.0261
Epoch 200/200
Epoch [200/200], Training Loss: 0.0044, Validation Loss: 0.0261
Test Loss: 0.0166
done with hyperparameter tuning training
Model type: UNet6, Epochs: 200, Learning rate: 0.0001, Batch size: 64
Hyperparameter tuning prediction finished
UNet6 with 0.4 0.3 200 0.0001 64 small done at Sat Nov  2 04:19:21 CET 2024
SBATCH job finished
