import torch
import torch.nn as nn

class HybridLSTM_CNN(nn.Module):
    def __init__(self, time_steps, spatial_dims, hidden_size):
        super(HybridLSTM_CNN, self).__init__()
        self.time_steps = time_steps
        self.spatial_dims = spatial_dims
        self.hidden_size = hidden_size

        print('spatial_dims:', spatial_dims)
        print('spatial_dims[0]:', spatial_dims[0])  
        print('spatial_dims[1]:', spatial_dims[1])
        # LSTM layer
        self.lstm = nn.LSTM(input_size=spatial_dims[0] * spatial_dims[1], 
                            hidden_size=hidden_size, 
                            batch_first=True)
        # Fully connected layer for continuous temporal prediction
        self.fc = nn.Linear(hidden_size, spatial_dims[0] * spatial_dims[1])  # Predict next time-step

        # CNN layers for spatial mapping
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()


    def forward(self, x, steps=1):
        # LSTM expects (batch_size, time_steps, features)
        batch_size = x.size(0)
        outputs = []
        spatial_outputs = []

        # Flatten spatial dimensions for LSTM input
        x_flat = x.view(batch_size, self.time_steps, -1)
        print('x_flat:', x_flat.shape)

        # Initialize LSTM hidden and cell states
        h_n = torch.zeros(1, batch_size, self.hidden_size)
        c_n = torch.zeros(1, batch_size, self.hidden_size)
        print('h_n:', h_n.shape)
        print('c_n:', c_n.shape)

        for step in range(steps):
            # if step == 0:
            lstm_out, (h_n, c_n) = self.lstm(x_flat, (h_n, c_n))
            print('lstm_out:', lstm_out.shape)
            # else:
            #     lstm_out, (h_n, c_n) = self.lstm(c_n.permute(1, 0, 2), (h_n, c_n))
            outputs.append(lstm_out)

            # Project LSTM cell state to spatial dimensions
            # c_n_projected = self.fc(c_n.squeeze(0))  # Shape: [batch_size, spatial_dims[0] * spatial_dims[1]]
            # print('c_n_projected:', c_n_projected.shape)
            cnn_input = lstm_out.view(batch_size, step, self.spatial_dims[0], self.spatial_dims[1])  # Shape: [batch_size, 1, 64, 64]
            print('cnn_input:', cnn_input.shape)
            # Pass through CNN to generate spatial map and update c_n
            cnn_out = self.conv1(cnn_input)
            cnn_out = self.relu(cnn_out)
            cnn_out = self.conv2(cnn_out)
            cnn_out = self.sigmoid(cnn_out)

            c_n = cnn_out.view(batch_size, -1).unsqueeze(0)  # Flatten and add batch dimension for LSTM
            print('c_n:', c_n.shape)
            spatial_outputs.append(cnn_out)

        return outputs, spatial_outputs

# Initialize model
time_steps = 10
spatial_dims = (64, 64)
hidden_size = 128
model = HybridLSTM_CNN(time_steps, spatial_dims, hidden_size)

# Example input
batch_size = 1
x = torch.rand(batch_size, time_steps, 1, spatial_dims[0], spatial_dims[1])  # (batch_size, time_steps, channels, height, width)

# Forward pass for continuous prediction (steps=5 for prediction into the future)
temporal_out, spatial_out = model(x, steps=5)

print("Temporal Output Shape:", temporal_out.shape)  # (batch_size, steps, spatial_dims[0]*spatial_dims[1])
print("Spatial Output Shape:", spatial_out.shape)    # (batch_size, steps, 1, spatial_dims[0], spatial_dims[1])
